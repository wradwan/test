{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, PReLU, Dropout, BatchNormalization,Conv2D\n",
    "\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first we will load MNIST dataset and try some Tricks\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000,784).astype('float32')\n",
    "X_test = X_test.reshape(10000,784).astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dropout is the most common way to reduce overfitting.\n",
    "#Here we are adding the same 5 hidden layers (Prelu activation)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(784, input_dim=784, kernel_initializer='normal'))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(400, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "3s - loss: 0.4649 - acc: 0.8541 - val_loss: 0.1266 - val_acc: 0.9640\n",
      "Epoch 2/100\n",
      "3s - loss: 0.1403 - acc: 0.9636 - val_loss: 0.1189 - val_acc: 0.9682\n",
      "Epoch 3/100\n",
      "3s - loss: 0.0955 - acc: 0.9754 - val_loss: 0.0905 - val_acc: 0.9745\n",
      "Epoch 4/100\n",
      "3s - loss: 0.0743 - acc: 0.9809 - val_loss: 0.0829 - val_acc: 0.9768\n",
      "Epoch 5/100\n",
      "3s - loss: 0.0630 - acc: 0.9838 - val_loss: 0.1008 - val_acc: 0.9746\n",
      "Epoch 6/100\n",
      "3s - loss: 0.0517 - acc: 0.9869 - val_loss: 0.0811 - val_acc: 0.9786\n",
      "Epoch 7/100\n",
      "3s - loss: 0.0431 - acc: 0.9886 - val_loss: 0.1044 - val_acc: 0.9766\n",
      "Epoch 8/100\n",
      "3s - loss: 0.0377 - acc: 0.9900 - val_loss: 0.0986 - val_acc: 0.9772\n",
      "Epoch 9/100\n",
      "3s - loss: 0.0353 - acc: 0.9906 - val_loss: 0.1019 - val_acc: 0.9781\n",
      "Epoch 10/100\n",
      "3s - loss: 0.0289 - acc: 0.9919 - val_loss: 0.1019 - val_acc: 0.9815\n",
      "Epoch 11/100\n",
      "3s - loss: 0.0268 - acc: 0.9929 - val_loss: 0.0994 - val_acc: 0.9805\n",
      "Epoch 12/100\n",
      "3s - loss: 0.0260 - acc: 0.9935 - val_loss: 0.1082 - val_acc: 0.9780\n",
      "Epoch 13/100\n",
      "3s - loss: 0.0235 - acc: 0.9938 - val_loss: 0.0955 - val_acc: 0.9819\n",
      "Epoch 14/100\n",
      "3s - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0899 - val_acc: 0.9827\n",
      "Epoch 15/100\n",
      "3s - loss: 0.0207 - acc: 0.9947 - val_loss: 0.1183 - val_acc: 0.9765\n",
      "Epoch 16/100\n",
      "3s - loss: 0.0200 - acc: 0.9948 - val_loss: 0.0920 - val_acc: 0.9842\n",
      "Epoch 17/100\n",
      "3s - loss: 0.0194 - acc: 0.9956 - val_loss: 0.0943 - val_acc: 0.9828\n",
      "Epoch 18/100\n",
      "3s - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0922 - val_acc: 0.9814\n",
      "Epoch 19/100\n",
      "3s - loss: 0.0159 - acc: 0.9959 - val_loss: 0.1046 - val_acc: 0.9787\n",
      "Epoch 20/100\n",
      "3s - loss: 0.0160 - acc: 0.9958 - val_loss: 0.1139 - val_acc: 0.9796\n",
      "Epoch 21/100\n",
      "3s - loss: 0.0200 - acc: 0.9951 - val_loss: 0.1053 - val_acc: 0.9823\n",
      "Epoch 22/100\n",
      "3s - loss: 0.0199 - acc: 0.9951 - val_loss: 0.1106 - val_acc: 0.9810\n",
      "Epoch 23/100\n",
      "3s - loss: 0.0119 - acc: 0.9970 - val_loss: 0.1192 - val_acc: 0.9822\n",
      "Epoch 24/100\n",
      "3s - loss: 0.0123 - acc: 0.9968 - val_loss: 0.1172 - val_acc: 0.9842\n",
      "Epoch 25/100\n",
      "3s - loss: 0.0112 - acc: 0.9973 - val_loss: 0.1083 - val_acc: 0.9819\n",
      "Epoch 26/100\n",
      "3s - loss: 0.0118 - acc: 0.9971 - val_loss: 0.1227 - val_acc: 0.9809\n",
      "Epoch 27/100\n",
      "3s - loss: 0.0141 - acc: 0.9965 - val_loss: 0.1074 - val_acc: 0.9817\n",
      "Epoch 28/100\n",
      "3s - loss: 0.0158 - acc: 0.9964 - val_loss: 0.0990 - val_acc: 0.9824\n",
      "Epoch 29/100\n",
      "3s - loss: 0.0122 - acc: 0.9972 - val_loss: 0.1018 - val_acc: 0.9850\n",
      "Epoch 30/100\n",
      "3s - loss: 0.0154 - acc: 0.9964 - val_loss: 0.1036 - val_acc: 0.9829\n",
      "Epoch 31/100\n",
      "3s - loss: 0.0130 - acc: 0.9968 - val_loss: 0.1124 - val_acc: 0.9828\n",
      "Epoch 32/100\n",
      "3s - loss: 0.0109 - acc: 0.9976 - val_loss: 0.1203 - val_acc: 0.9824\n",
      "Epoch 33/100\n",
      "3s - loss: 0.0107 - acc: 0.9971 - val_loss: 0.1087 - val_acc: 0.9838\n",
      "Epoch 34/100\n",
      "3s - loss: 0.0107 - acc: 0.9973 - val_loss: 0.1075 - val_acc: 0.9835\n",
      "Epoch 35/100\n",
      "3s - loss: 0.0113 - acc: 0.9973 - val_loss: 0.1192 - val_acc: 0.9838\n",
      "Epoch 36/100\n",
      "3s - loss: 0.0102 - acc: 0.9976 - val_loss: 0.1232 - val_acc: 0.9836\n",
      "Epoch 37/100\n",
      "3s - loss: 0.0092 - acc: 0.9978 - val_loss: 0.1089 - val_acc: 0.9828\n",
      "Epoch 38/100\n",
      "3s - loss: 0.0101 - acc: 0.9977 - val_loss: 0.1270 - val_acc: 0.9811\n",
      "Epoch 39/100\n",
      "3s - loss: 0.0081 - acc: 0.9980 - val_loss: 0.1331 - val_acc: 0.9813\n",
      "Epoch 40/100\n",
      "3s - loss: 0.0152 - acc: 0.9965 - val_loss: 0.1157 - val_acc: 0.9843\n",
      "Epoch 41/100\n",
      "3s - loss: 0.0103 - acc: 0.9976 - val_loss: 0.1113 - val_acc: 0.9830\n",
      "Epoch 42/100\n",
      "3s - loss: 0.0074 - acc: 0.9981 - val_loss: 0.1304 - val_acc: 0.9840\n",
      "Epoch 43/100\n",
      "3s - loss: 0.0079 - acc: 0.9982 - val_loss: 0.1197 - val_acc: 0.9826\n",
      "Epoch 44/100\n",
      "3s - loss: 0.0106 - acc: 0.9976 - val_loss: 0.1205 - val_acc: 0.9833\n",
      "Epoch 45/100\n",
      "3s - loss: 0.0101 - acc: 0.9977 - val_loss: 0.1550 - val_acc: 0.9796\n",
      "Epoch 46/100\n",
      "3s - loss: 0.0118 - acc: 0.9975 - val_loss: 0.1118 - val_acc: 0.9842\n",
      "Epoch 47/100\n",
      "3s - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1340 - val_acc: 0.9832\n",
      "Epoch 48/100\n",
      "3s - loss: 0.0093 - acc: 0.9978 - val_loss: 0.1155 - val_acc: 0.9846\n",
      "Epoch 49/100\n",
      "3s - loss: 0.0077 - acc: 0.9985 - val_loss: 0.1245 - val_acc: 0.9830\n",
      "Epoch 50/100\n",
      "3s - loss: 0.0087 - acc: 0.9980 - val_loss: 0.1337 - val_acc: 0.9800\n",
      "Epoch 51/100\n",
      "3s - loss: 0.0080 - acc: 0.9985 - val_loss: 0.1272 - val_acc: 0.9820\n",
      "Epoch 52/100\n",
      "3s - loss: 0.0131 - acc: 0.9973 - val_loss: 0.1336 - val_acc: 0.9813\n",
      "Epoch 53/100\n",
      "3s - loss: 0.0124 - acc: 0.9974 - val_loss: 0.1145 - val_acc: 0.9828\n",
      "Epoch 54/100\n",
      "3s - loss: 0.0071 - acc: 0.9984 - val_loss: 0.1397 - val_acc: 0.9824\n",
      "Epoch 55/100\n",
      "3s - loss: 0.0082 - acc: 0.9985 - val_loss: 0.1225 - val_acc: 0.9831\n",
      "Epoch 56/100\n",
      "3s - loss: 0.0099 - acc: 0.9977 - val_loss: 0.1346 - val_acc: 0.9812\n",
      "Epoch 57/100\n",
      "3s - loss: 0.0078 - acc: 0.9984 - val_loss: 0.1235 - val_acc: 0.9814\n",
      "Epoch 58/100\n",
      "3s - loss: 0.0141 - acc: 0.9971 - val_loss: 0.1090 - val_acc: 0.9831\n",
      "Epoch 59/100\n",
      "3s - loss: 0.0059 - acc: 0.9988 - val_loss: 0.1316 - val_acc: 0.9828\n",
      "Epoch 60/100\n",
      "3s - loss: 0.0079 - acc: 0.9983 - val_loss: 0.1250 - val_acc: 0.9835\n",
      "Epoch 61/100\n",
      "3s - loss: 0.0086 - acc: 0.9981 - val_loss: 0.1376 - val_acc: 0.9781\n",
      "Epoch 62/100\n",
      "3s - loss: 0.0095 - acc: 0.9980 - val_loss: 0.1158 - val_acc: 0.9837\n",
      "Epoch 63/100\n",
      "3s - loss: 0.0052 - acc: 0.9989 - val_loss: 0.1445 - val_acc: 0.9836\n",
      "Epoch 64/100\n",
      "3s - loss: 0.0153 - acc: 0.9971 - val_loss: 0.1366 - val_acc: 0.9800\n",
      "Epoch 65/100\n",
      "3s - loss: 0.0069 - acc: 0.9984 - val_loss: 0.1345 - val_acc: 0.9828\n",
      "Epoch 66/100\n",
      "3s - loss: 0.0099 - acc: 0.9981 - val_loss: 0.1346 - val_acc: 0.9829\n",
      "Epoch 67/100\n",
      "3s - loss: 0.0061 - acc: 0.9987 - val_loss: 0.1326 - val_acc: 0.9831\n",
      "Epoch 68/100\n",
      "3s - loss: 0.0073 - acc: 0.9986 - val_loss: 0.1597 - val_acc: 0.9808\n",
      "Epoch 69/100\n",
      "3s - loss: 0.0086 - acc: 0.9983 - val_loss: 0.1312 - val_acc: 0.9830\n",
      "Epoch 70/100\n",
      "3s - loss: 0.0088 - acc: 0.9983 - val_loss: 0.1300 - val_acc: 0.9831\n",
      "Epoch 71/100\n",
      "3s - loss: 0.0055 - acc: 0.9988 - val_loss: 0.1366 - val_acc: 0.9840\n",
      "Epoch 72/100\n",
      "3s - loss: 0.0049 - acc: 0.9991 - val_loss: 0.1223 - val_acc: 0.9833\n",
      "Epoch 73/100\n",
      "3s - loss: 0.0057 - acc: 0.9990 - val_loss: 0.1194 - val_acc: 0.9843\n",
      "Epoch 74/100\n",
      "3s - loss: 0.0107 - acc: 0.9980 - val_loss: 0.1363 - val_acc: 0.9824\n",
      "Epoch 75/100\n",
      "3s - loss: 0.0087 - acc: 0.9983 - val_loss: 0.1240 - val_acc: 0.9838\n",
      "Epoch 76/100\n",
      "3s - loss: 0.0043 - acc: 0.9991 - val_loss: 0.1300 - val_acc: 0.9849\n",
      "Epoch 77/100\n",
      "3s - loss: 0.0050 - acc: 0.9990 - val_loss: 0.1251 - val_acc: 0.9840\n",
      "Epoch 78/100\n",
      "3s - loss: 0.0155 - acc: 0.9972 - val_loss: 0.1262 - val_acc: 0.9851\n",
      "Epoch 79/100\n",
      "3s - loss: 0.0051 - acc: 0.9991 - val_loss: 0.1357 - val_acc: 0.9844\n",
      "Epoch 80/100\n",
      "3s - loss: 0.0112 - acc: 0.9983 - val_loss: 0.1574 - val_acc: 0.9823\n",
      "Epoch 81/100\n",
      "3s - loss: 0.0116 - acc: 0.9978 - val_loss: 0.1414 - val_acc: 0.9827\n",
      "Epoch 82/100\n",
      "3s - loss: 0.0130 - acc: 0.9975 - val_loss: 0.1423 - val_acc: 0.9822\n",
      "Epoch 83/100\n",
      "3s - loss: 0.0053 - acc: 0.9990 - val_loss: 0.1345 - val_acc: 0.9838\n",
      "Epoch 84/100\n",
      "3s - loss: 0.0056 - acc: 0.9988 - val_loss: 0.1358 - val_acc: 0.9842\n",
      "Epoch 85/100\n",
      "3s - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1282 - val_acc: 0.9848\n",
      "Epoch 86/100\n",
      "3s - loss: 0.0046 - acc: 0.9992 - val_loss: 0.1379 - val_acc: 0.9842\n",
      "Epoch 87/100\n",
      "3s - loss: 0.0139 - acc: 0.9975 - val_loss: 0.1558 - val_acc: 0.9829\n",
      "Epoch 88/100\n",
      "3s - loss: 0.0092 - acc: 0.9983 - val_loss: 0.1327 - val_acc: 0.9825\n",
      "Epoch 89/100\n",
      "3s - loss: 0.0108 - acc: 0.9981 - val_loss: 0.1633 - val_acc: 0.9797\n",
      "Epoch 90/100\n",
      "3s - loss: 0.0085 - acc: 0.9983 - val_loss: 0.1244 - val_acc: 0.9836\n",
      "Epoch 91/100\n",
      "3s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1375 - val_acc: 0.9857\n",
      "Epoch 92/100\n",
      "3s - loss: 0.0042 - acc: 0.9993 - val_loss: 0.1279 - val_acc: 0.9858\n",
      "Epoch 93/100\n",
      "3s - loss: 0.0047 - acc: 0.9991 - val_loss: 0.1370 - val_acc: 0.9829\n",
      "Epoch 94/100\n",
      "3s - loss: 0.0087 - acc: 0.9983 - val_loss: 0.1351 - val_acc: 0.9856\n",
      "Epoch 95/100\n",
      "3s - loss: 0.0049 - acc: 0.9992 - val_loss: 0.1423 - val_acc: 0.9827\n",
      "Epoch 96/100\n",
      "3s - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1435 - val_acc: 0.9845\n",
      "Epoch 97/100\n",
      "3s - loss: 0.0034 - acc: 0.9994 - val_loss: 0.1587 - val_acc: 0.9833\n",
      "Epoch 98/100\n",
      "3s - loss: 0.0085 - acc: 0.9984 - val_loss: 0.1339 - val_acc: 0.9859\n",
      "Epoch 99/100\n",
      "3s - loss: 0.0090 - acc: 0.9982 - val_loss: 0.1621 - val_acc: 0.9826\n",
      "Epoch 100/100\n",
      "3s - loss: 0.0081 - acc: 0.9985 - val_loss: 0.1294 - val_acc: 0.9846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75adec7e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/model_5_layers_dropout')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9792/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe score of this model is %98.4600 \n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "final_score = scores[1]*100\n",
    "print ('The score of this model is %%%.4f ' % final_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BatchNormalization is the most common way to reduce overfitting.\n",
    "#Here we are adding the same 5 hidden layers (Prelu activation)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(784, input_dim=784, kernel_initializer='normal'))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(400, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "6s - loss: 0.4154 - acc: 0.8822 - val_loss: 0.1635 - val_acc: 0.9538\n",
      "Epoch 2/100\n",
      "6s - loss: 0.1472 - acc: 0.9576 - val_loss: 0.1279 - val_acc: 0.9642\n",
      "Epoch 3/100\n",
      "6s - loss: 0.1038 - acc: 0.9707 - val_loss: 0.1251 - val_acc: 0.9653\n",
      "Epoch 4/100\n",
      "6s - loss: 0.0824 - acc: 0.9759 - val_loss: 0.0949 - val_acc: 0.9750\n",
      "Epoch 5/100\n",
      "6s - loss: 0.0688 - acc: 0.9798 - val_loss: 0.0900 - val_acc: 0.9752\n",
      "Epoch 6/100\n",
      "6s - loss: 0.0576 - acc: 0.9839 - val_loss: 0.0801 - val_acc: 0.9770\n",
      "Epoch 7/100\n",
      "6s - loss: 0.0493 - acc: 0.9859 - val_loss: 0.0782 - val_acc: 0.9785\n",
      "Epoch 8/100\n",
      "6s - loss: 0.0428 - acc: 0.9875 - val_loss: 0.0759 - val_acc: 0.9805\n",
      "Epoch 9/100\n",
      "6s - loss: 0.0430 - acc: 0.9875 - val_loss: 0.0796 - val_acc: 0.9785\n",
      "Epoch 10/100\n",
      "6s - loss: 0.0340 - acc: 0.9902 - val_loss: 0.0741 - val_acc: 0.9811\n",
      "Epoch 11/100\n",
      "6s - loss: 0.0299 - acc: 0.9909 - val_loss: 0.0833 - val_acc: 0.9799\n",
      "Epoch 12/100\n",
      "6s - loss: 0.0313 - acc: 0.9905 - val_loss: 0.0916 - val_acc: 0.9776\n",
      "Epoch 13/100\n",
      "6s - loss: 0.0281 - acc: 0.9918 - val_loss: 0.0894 - val_acc: 0.9782\n",
      "Epoch 14/100\n",
      "6s - loss: 0.0255 - acc: 0.9927 - val_loss: 0.0916 - val_acc: 0.9777\n",
      "Epoch 15/100\n",
      "6s - loss: 0.0251 - acc: 0.9929 - val_loss: 0.0879 - val_acc: 0.9783\n",
      "Epoch 16/100\n",
      "6s - loss: 0.0197 - acc: 0.9941 - val_loss: 0.0787 - val_acc: 0.9828\n",
      "Epoch 17/100\n",
      "6s - loss: 0.0207 - acc: 0.9943 - val_loss: 0.1105 - val_acc: 0.9762\n",
      "Epoch 18/100\n",
      "6s - loss: 0.0201 - acc: 0.9943 - val_loss: 0.1003 - val_acc: 0.9784\n",
      "Epoch 19/100\n",
      "6s - loss: 0.0175 - acc: 0.9948 - val_loss: 0.0801 - val_acc: 0.9824\n",
      "Epoch 20/100\n",
      "6s - loss: 0.0177 - acc: 0.9950 - val_loss: 0.0854 - val_acc: 0.9795\n",
      "Epoch 21/100\n",
      "6s - loss: 0.0180 - acc: 0.9945 - val_loss: 0.0782 - val_acc: 0.9819\n",
      "Epoch 22/100\n",
      "6s - loss: 0.0164 - acc: 0.9951 - val_loss: 0.0870 - val_acc: 0.9798\n",
      "Epoch 23/100\n",
      "6s - loss: 0.0177 - acc: 0.9948 - val_loss: 0.0788 - val_acc: 0.9818\n",
      "Epoch 24/100\n",
      "6s - loss: 0.0130 - acc: 0.9962 - val_loss: 0.0949 - val_acc: 0.9795\n",
      "Epoch 25/100\n",
      "6s - loss: 0.0141 - acc: 0.9960 - val_loss: 0.1015 - val_acc: 0.9787\n",
      "Epoch 26/100\n",
      "6s - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0803 - val_acc: 0.9817\n",
      "Epoch 27/100\n",
      "6s - loss: 0.0130 - acc: 0.9962 - val_loss: 0.0864 - val_acc: 0.9821\n",
      "Epoch 28/100\n",
      "6s - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0766 - val_acc: 0.9828\n",
      "Epoch 29/100\n",
      "6s - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0913 - val_acc: 0.9821\n",
      "Epoch 30/100\n",
      "6s - loss: 0.0121 - acc: 0.9966 - val_loss: 0.0814 - val_acc: 0.9833\n",
      "Epoch 31/100\n",
      "6s - loss: 0.0103 - acc: 0.9971 - val_loss: 0.0788 - val_acc: 0.9839\n",
      "Epoch 32/100\n",
      "6s - loss: 0.0114 - acc: 0.9969 - val_loss: 0.0918 - val_acc: 0.9817\n",
      "Epoch 33/100\n",
      "6s - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0880 - val_acc: 0.9823\n",
      "Epoch 34/100\n",
      "6s - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0821 - val_acc: 0.9835\n",
      "Epoch 35/100\n",
      "6s - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0800 - val_acc: 0.9835\n",
      "Epoch 36/100\n",
      "6s - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0811 - val_acc: 0.9840\n",
      "Epoch 37/100\n",
      "6s - loss: 0.0075 - acc: 0.9979 - val_loss: 0.0998 - val_acc: 0.9806\n",
      "Epoch 38/100\n",
      "6s - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0878 - val_acc: 0.9827\n",
      "Epoch 39/100\n",
      "6s - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0886 - val_acc: 0.9821\n",
      "Epoch 40/100\n",
      "6s - loss: 0.0088 - acc: 0.9977 - val_loss: 0.0863 - val_acc: 0.9828\n",
      "Epoch 41/100\n",
      "6s - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0911 - val_acc: 0.9818\n",
      "Epoch 42/100\n",
      "6s - loss: 0.0095 - acc: 0.9973 - val_loss: 0.0800 - val_acc: 0.9817\n",
      "Epoch 43/100\n",
      "6s - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0832 - val_acc: 0.9836\n",
      "Epoch 44/100\n",
      "6s - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0870 - val_acc: 0.9845\n",
      "Epoch 45/100\n",
      "6s - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0856 - val_acc: 0.9825\n",
      "Epoch 46/100\n",
      "6s - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0867 - val_acc: 0.9833\n",
      "Epoch 47/100\n",
      "6s - loss: 0.0083 - acc: 0.9977 - val_loss: 0.0907 - val_acc: 0.9821\n",
      "Epoch 48/100\n",
      "6s - loss: 0.0089 - acc: 0.9977 - val_loss: 0.0726 - val_acc: 0.9846\n",
      "Epoch 49/100\n",
      "6s - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0834 - val_acc: 0.9840\n",
      "Epoch 50/100\n",
      "6s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0917 - val_acc: 0.9826\n",
      "Epoch 51/100\n",
      "6s - loss: 0.0079 - acc: 0.9975 - val_loss: 0.0887 - val_acc: 0.9825\n",
      "Epoch 52/100\n",
      "6s - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0854 - val_acc: 0.9831\n",
      "Epoch 53/100\n",
      "6s - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0960 - val_acc: 0.9822\n",
      "Epoch 54/100\n",
      "6s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0938 - val_acc: 0.9807\n",
      "Epoch 55/100\n",
      "6s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0757 - val_acc: 0.9855\n",
      "Epoch 56/100\n",
      "6s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0808 - val_acc: 0.9852\n",
      "Epoch 57/100\n",
      "6s - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0948 - val_acc: 0.9837\n",
      "Epoch 58/100\n",
      "6s - loss: 0.0072 - acc: 0.9980 - val_loss: 0.0911 - val_acc: 0.9830\n",
      "Epoch 59/100\n",
      "6s - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0780 - val_acc: 0.9849\n",
      "Epoch 60/100\n",
      "6s - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0818 - val_acc: 0.9841\n",
      "Epoch 61/100\n",
      "6s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0907 - val_acc: 0.9844\n",
      "Epoch 62/100\n",
      "6s - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0828 - val_acc: 0.9854\n",
      "Epoch 63/100\n",
      "6s - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0887 - val_acc: 0.9853\n",
      "Epoch 64/100\n",
      "6s - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0922 - val_acc: 0.9850\n",
      "Epoch 65/100\n",
      "6s - loss: 0.0039 - acc: 0.9988 - val_loss: 0.1233 - val_acc: 0.9795\n",
      "Epoch 66/100\n",
      "6s - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0926 - val_acc: 0.9840\n",
      "Epoch 67/100\n",
      "6s - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0905 - val_acc: 0.9838\n",
      "Epoch 68/100\n",
      "6s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0821 - val_acc: 0.9852\n",
      "Epoch 69/100\n",
      "6s - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0939 - val_acc: 0.9819\n",
      "Epoch 70/100\n",
      "6s - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0896 - val_acc: 0.9842\n",
      "Epoch 71/100\n",
      "6s - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0957 - val_acc: 0.9815\n",
      "Epoch 72/100\n",
      "6s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0873 - val_acc: 0.9854\n",
      "Epoch 73/100\n",
      "6s - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0897 - val_acc: 0.9841\n",
      "Epoch 74/100\n",
      "6s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0930 - val_acc: 0.9836\n",
      "Epoch 75/100\n",
      "6s - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0885 - val_acc: 0.9845\n",
      "Epoch 76/100\n",
      "6s - loss: 0.0054 - acc: 0.9986 - val_loss: 0.0866 - val_acc: 0.9838\n",
      "Epoch 77/100\n",
      "6s - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0890 - val_acc: 0.9846\n",
      "Epoch 78/100\n",
      "6s - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0984 - val_acc: 0.9821\n",
      "Epoch 79/100\n",
      "6s - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1099 - val_acc: 0.9823\n",
      "Epoch 80/100\n",
      "6s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0951 - val_acc: 0.9830\n",
      "Epoch 81/100\n",
      "6s - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0874 - val_acc: 0.9843\n",
      "Epoch 82/100\n",
      "6s - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0898 - val_acc: 0.9850\n",
      "Epoch 83/100\n",
      "6s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0919 - val_acc: 0.9841\n",
      "Epoch 84/100\n",
      "6s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0847 - val_acc: 0.9850\n",
      "Epoch 85/100\n",
      "6s - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0902 - val_acc: 0.9852\n",
      "Epoch 86/100\n",
      "6s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1165 - val_acc: 0.9816\n",
      "Epoch 87/100\n",
      "6s - loss: 0.0068 - acc: 0.9984 - val_loss: 0.0885 - val_acc: 0.9838\n",
      "Epoch 88/100\n",
      "6s - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0861 - val_acc: 0.9848\n",
      "Epoch 89/100\n",
      "6s - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1138 - val_acc: 0.9810\n",
      "Epoch 90/100\n",
      "6s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0910 - val_acc: 0.9841\n",
      "Epoch 91/100\n",
      "6s - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0835 - val_acc: 0.9856\n",
      "Epoch 92/100\n",
      "6s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0962 - val_acc: 0.9841\n",
      "Epoch 93/100\n",
      "6s - loss: 0.0027 - acc: 0.9992 - val_loss: 0.1030 - val_acc: 0.9842\n",
      "Epoch 94/100\n",
      "6s - loss: 0.0054 - acc: 0.9986 - val_loss: 0.0891 - val_acc: 0.9848\n",
      "Epoch 95/100\n",
      "6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0988 - val_acc: 0.9838\n",
      "Epoch 96/100\n",
      "6s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1109 - val_acc: 0.9828\n",
      "Epoch 97/100\n",
      "6s - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1014 - val_acc: 0.9829\n",
      "Epoch 98/100\n",
      "6s - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0945 - val_acc: 0.9834\n",
      "Epoch 99/100\n",
      "6s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1069 - val_acc: 0.9810\n",
      "Epoch 100/100\n",
      "6s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1034 - val_acc: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f754fc0ec50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/model_5_layers_BatchNormalization')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9888/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe score of this model is %98.3400 \n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "final_score = scores[1]*100\n",
    "print ('The score of this model is %%%.4f ' % final_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use with other dataset (CIFAR10)\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75fc6c24d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(50000,3072).astype('float32')\n",
    "X_test = X_test.reshape(10000,3072).astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Here we are adding 1 hidden layer with relu activiation \n",
    "#still not a NN\n",
    "model = Sequential()\n",
    "model.add(Dense(3072, kernel_initializer=\"normal\", input_dim=3072))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "5s - loss: 2.3554 - acc: 0.1221 - val_loss: 2.1805 - val_acc: 0.1629\n",
      "Epoch 2/100\n",
      "4s - loss: 2.1633 - acc: 0.1686 - val_loss: 2.1374 - val_acc: 0.1815\n",
      "Epoch 3/100\n",
      "4s - loss: 2.1388 - acc: 0.1697 - val_loss: 2.1197 - val_acc: 0.1793\n",
      "Epoch 4/100\n",
      "4s - loss: 2.1127 - acc: 0.1728 - val_loss: 2.1067 - val_acc: 0.1678\n",
      "Epoch 5/100\n",
      "4s - loss: 2.1032 - acc: 0.1736 - val_loss: 2.0986 - val_acc: 0.1815\n",
      "Epoch 6/100\n",
      "4s - loss: 2.0931 - acc: 0.1801 - val_loss: 2.0819 - val_acc: 0.1880\n",
      "Epoch 7/100\n",
      "4s - loss: 2.0855 - acc: 0.1818 - val_loss: 2.0719 - val_acc: 0.1870\n",
      "Epoch 8/100\n",
      "4s - loss: 2.1017 - acc: 0.1771 - val_loss: 2.1123 - val_acc: 0.1644\n",
      "Epoch 9/100\n",
      "4s - loss: 2.0963 - acc: 0.1767 - val_loss: 2.0885 - val_acc: 0.1764\n",
      "Epoch 10/100\n",
      "4s - loss: 2.0817 - acc: 0.1804 - val_loss: 2.0703 - val_acc: 0.1841\n",
      "Epoch 11/100\n",
      "4s - loss: 2.0792 - acc: 0.1824 - val_loss: 2.0811 - val_acc: 0.1752\n",
      "Epoch 12/100\n",
      "4s - loss: 2.0783 - acc: 0.1796 - val_loss: 2.0652 - val_acc: 0.1741\n",
      "Epoch 13/100\n",
      "4s - loss: 2.0783 - acc: 0.1786 - val_loss: 2.0613 - val_acc: 0.1879\n",
      "Epoch 14/100\n",
      "4s - loss: 2.0739 - acc: 0.1801 - val_loss: 2.0610 - val_acc: 0.1828\n",
      "Epoch 15/100\n",
      "4s - loss: 2.0692 - acc: 0.1803 - val_loss: 2.0588 - val_acc: 0.1815\n",
      "Epoch 16/100\n",
      "4s - loss: 2.0649 - acc: 0.1817 - val_loss: 2.0533 - val_acc: 0.1840\n",
      "Epoch 17/100\n",
      "4s - loss: 2.0659 - acc: 0.1811 - val_loss: 2.0612 - val_acc: 0.1738\n",
      "Epoch 18/100\n",
      "4s - loss: 2.0691 - acc: 0.1806 - val_loss: 2.0693 - val_acc: 0.1701\n",
      "Epoch 19/100\n",
      "4s - loss: 2.0615 - acc: 0.1785 - val_loss: 2.1046 - val_acc: 0.1747\n",
      "Epoch 20/100\n",
      "4s - loss: 2.0653 - acc: 0.1838 - val_loss: 2.0500 - val_acc: 0.1880\n",
      "Epoch 21/100\n",
      "4s - loss: 2.0641 - acc: 0.1804 - val_loss: 2.0504 - val_acc: 0.1843\n",
      "Epoch 22/100\n",
      "4s - loss: 2.0558 - acc: 0.1820 - val_loss: 2.0669 - val_acc: 0.1872\n",
      "Epoch 23/100\n",
      "4s - loss: 2.0776 - acc: 0.1817 - val_loss: 2.0787 - val_acc: 0.1634\n",
      "Epoch 24/100\n",
      "4s - loss: 2.0645 - acc: 0.1833 - val_loss: 2.0512 - val_acc: 0.1868\n",
      "Epoch 25/100\n",
      "4s - loss: 2.0575 - acc: 0.1845 - val_loss: 2.0556 - val_acc: 0.1847\n",
      "Epoch 26/100\n",
      "4s - loss: 2.0663 - acc: 0.1818 - val_loss: 2.0856 - val_acc: 0.1769\n",
      "Epoch 27/100\n",
      "4s - loss: 2.0556 - acc: 0.1840 - val_loss: 2.0580 - val_acc: 0.1680\n",
      "Epoch 28/100\n",
      "4s - loss: 2.0625 - acc: 0.1831 - val_loss: 2.0737 - val_acc: 0.1699\n",
      "Epoch 29/100\n",
      "4s - loss: 2.0529 - acc: 0.1841 - val_loss: 2.0443 - val_acc: 0.1829\n",
      "Epoch 30/100\n",
      "4s - loss: 2.0570 - acc: 0.1828 - val_loss: 2.0475 - val_acc: 0.1879\n",
      "Epoch 31/100\n",
      "4s - loss: 2.0568 - acc: 0.1824 - val_loss: 2.0446 - val_acc: 0.1868\n",
      "Epoch 32/100\n",
      "4s - loss: 2.0492 - acc: 0.1850 - val_loss: 2.0433 - val_acc: 0.1845\n",
      "Epoch 33/100\n",
      "4s - loss: 2.0537 - acc: 0.1837 - val_loss: 2.0529 - val_acc: 0.1894\n",
      "Epoch 34/100\n",
      "4s - loss: 2.0517 - acc: 0.1852 - val_loss: 2.0419 - val_acc: 0.1800\n",
      "Epoch 35/100\n",
      "4s - loss: 2.0547 - acc: 0.1840 - val_loss: 2.0509 - val_acc: 0.1874\n",
      "Epoch 36/100\n",
      "4s - loss: 2.0480 - acc: 0.1884 - val_loss: 2.0539 - val_acc: 0.1832\n",
      "Epoch 37/100\n",
      "4s - loss: 2.0468 - acc: 0.1835 - val_loss: 2.0814 - val_acc: 0.1778\n",
      "Epoch 38/100\n",
      "4s - loss: 2.0457 - acc: 0.1841 - val_loss: 2.0392 - val_acc: 0.1883\n",
      "Epoch 39/100\n",
      "4s - loss: 2.0473 - acc: 0.1822 - val_loss: 2.0883 - val_acc: 0.1857\n",
      "Epoch 40/100\n",
      "4s - loss: 2.0682 - acc: 0.1825 - val_loss: 2.0415 - val_acc: 0.1889\n",
      "Epoch 41/100\n",
      "4s - loss: 2.0588 - acc: 0.1833 - val_loss: 2.0480 - val_acc: 0.1865\n",
      "Epoch 42/100\n",
      "4s - loss: 2.0497 - acc: 0.1834 - val_loss: 2.0430 - val_acc: 0.1825\n",
      "Epoch 43/100\n",
      "4s - loss: 2.0483 - acc: 0.1842 - val_loss: 2.0485 - val_acc: 0.1788\n",
      "Epoch 44/100\n",
      "4s - loss: 2.0458 - acc: 0.1870 - val_loss: 2.0392 - val_acc: 0.1847\n",
      "Epoch 45/100\n",
      "4s - loss: 2.0445 - acc: 0.1866 - val_loss: 2.0469 - val_acc: 0.1877\n",
      "Epoch 46/100\n",
      "4s - loss: 2.0442 - acc: 0.1864 - val_loss: 2.0379 - val_acc: 0.1938\n",
      "Epoch 47/100\n",
      "4s - loss: 2.0418 - acc: 0.1859 - val_loss: 2.0393 - val_acc: 0.1927\n",
      "Epoch 48/100\n",
      "4s - loss: 2.0460 - acc: 0.1876 - val_loss: 2.0437 - val_acc: 0.1903\n",
      "Epoch 49/100\n",
      "4s - loss: 2.0438 - acc: 0.1863 - val_loss: 2.0372 - val_acc: 0.1918\n",
      "Epoch 50/100\n",
      "4s - loss: 2.0371 - acc: 0.1881 - val_loss: 2.0484 - val_acc: 0.1772\n",
      "Epoch 51/100\n",
      "4s - loss: 2.0416 - acc: 0.1861 - val_loss: 2.0446 - val_acc: 0.1859\n",
      "Epoch 52/100\n",
      "4s - loss: 2.0392 - acc: 0.1877 - val_loss: 2.0432 - val_acc: 0.1869\n",
      "Epoch 53/100\n",
      "4s - loss: 2.1298 - acc: 0.1840 - val_loss: 2.0370 - val_acc: 0.1828\n",
      "Epoch 54/100\n",
      "4s - loss: 2.0567 - acc: 0.1830 - val_loss: 2.0782 - val_acc: 0.1780\n",
      "Epoch 55/100\n",
      "4s - loss: 2.0490 - acc: 0.1839 - val_loss: 2.0714 - val_acc: 0.1876\n",
      "Epoch 56/100\n",
      "4s - loss: 2.0456 - acc: 0.1870 - val_loss: 2.0922 - val_acc: 0.1758\n",
      "Epoch 57/100\n",
      "4s - loss: 2.0500 - acc: 0.1848 - val_loss: 2.0428 - val_acc: 0.1779\n",
      "Epoch 58/100\n",
      "4s - loss: 2.0403 - acc: 0.1843 - val_loss: 2.0477 - val_acc: 0.1935\n",
      "Epoch 59/100\n",
      "4s - loss: 2.0396 - acc: 0.1877 - val_loss: 2.0397 - val_acc: 0.1899\n",
      "Epoch 60/100\n",
      "4s - loss: 2.0445 - acc: 0.1865 - val_loss: 2.0353 - val_acc: 0.1878\n",
      "Epoch 61/100\n",
      "4s - loss: 2.0440 - acc: 0.1862 - val_loss: 2.0408 - val_acc: 0.1900\n",
      "Epoch 62/100\n",
      "4s - loss: 2.0432 - acc: 0.1871 - val_loss: 2.0922 - val_acc: 0.1680\n",
      "Epoch 63/100\n",
      "4s - loss: 7.2229 - acc: 0.1486 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 64/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 65/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 66/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 67/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 68/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 69/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 70/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 71/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 72/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 73/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 74/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 75/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 76/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 77/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 78/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 79/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 80/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 81/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 82/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 83/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 84/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 85/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 86/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 87/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 88/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 89/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 90/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 91/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 92/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 93/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 94/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 95/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 96/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 97/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 98/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 99/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 100/100\n",
      "4s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f759de5f6d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/CIFAR10_1_hidden_layer')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropout is the most common way to reduce overfitting.\n",
    "#Here we are adding the same 5 hidden layers (Prelu activation)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(3072, kernel_initializer=\"normal\", input_dim=3072))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(400, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "8s - loss: 1.9337 - acc: 0.2935 - val_loss: 2.2646 - val_acc: 0.1726\n",
      "Epoch 2/100\n",
      "7s - loss: 1.7524 - acc: 0.3670 - val_loss: 1.9259 - val_acc: 0.3040\n",
      "Epoch 3/100\n",
      "7s - loss: 1.6870 - acc: 0.3941 - val_loss: 2.1637 - val_acc: 0.1873\n",
      "Epoch 4/100\n",
      "7s - loss: 1.6389 - acc: 0.4125 - val_loss: 1.7171 - val_acc: 0.3823\n",
      "Epoch 5/100\n",
      "7s - loss: 1.5855 - acc: 0.4309 - val_loss: 1.9449 - val_acc: 0.3126\n",
      "Epoch 6/100\n",
      "7s - loss: 1.5506 - acc: 0.4439 - val_loss: 1.9341 - val_acc: 0.3098\n",
      "Epoch 7/100\n",
      "7s - loss: 1.5221 - acc: 0.4537 - val_loss: 2.2344 - val_acc: 0.2266\n",
      "Epoch 8/100\n",
      "8s - loss: 1.5026 - acc: 0.4637 - val_loss: 2.2579 - val_acc: 0.2434\n",
      "Epoch 9/100\n",
      "7s - loss: 1.4909 - acc: 0.4661 - val_loss: 1.6769 - val_acc: 0.3848\n",
      "Epoch 10/100\n",
      "7s - loss: 1.4506 - acc: 0.4815 - val_loss: 1.6864 - val_acc: 0.3830\n",
      "Epoch 11/100\n",
      "7s - loss: 1.4391 - acc: 0.4869 - val_loss: 1.8102 - val_acc: 0.3338\n",
      "Epoch 12/100\n",
      "7s - loss: 1.4348 - acc: 0.4905 - val_loss: 2.3267 - val_acc: 0.2326\n",
      "Epoch 13/100\n",
      "7s - loss: 1.4342 - acc: 0.4896 - val_loss: 1.8950 - val_acc: 0.3200\n",
      "Epoch 14/100\n",
      "7s - loss: 1.4123 - acc: 0.4983 - val_loss: 1.8838 - val_acc: 0.3581\n",
      "Epoch 15/100\n",
      "7s - loss: 1.3841 - acc: 0.5089 - val_loss: 1.4788 - val_acc: 0.4701\n",
      "Epoch 16/100\n",
      "7s - loss: 1.3599 - acc: 0.5188 - val_loss: 1.6173 - val_acc: 0.4355\n",
      "Epoch 17/100\n",
      "7s - loss: 1.3446 - acc: 0.5239 - val_loss: 1.5206 - val_acc: 0.4606\n",
      "Epoch 18/100\n",
      "7s - loss: 1.3308 - acc: 0.5271 - val_loss: 1.6169 - val_acc: 0.4238\n",
      "Epoch 19/100\n",
      "7s - loss: 1.3248 - acc: 0.5290 - val_loss: 1.6962 - val_acc: 0.4074\n",
      "Epoch 20/100\n",
      "7s - loss: 1.3117 - acc: 0.5348 - val_loss: 1.9151 - val_acc: 0.3443\n",
      "Epoch 21/100\n",
      "7s - loss: 1.3008 - acc: 0.5368 - val_loss: 1.5020 - val_acc: 0.4526\n",
      "Epoch 22/100\n",
      "7s - loss: 1.3211 - acc: 0.5300 - val_loss: 1.8972 - val_acc: 0.3348\n",
      "Epoch 23/100\n",
      "7s - loss: 1.3061 - acc: 0.5350 - val_loss: 1.8104 - val_acc: 0.3816\n",
      "Epoch 24/100\n",
      "7s - loss: 1.2925 - acc: 0.5403 - val_loss: 1.6058 - val_acc: 0.4159\n",
      "Epoch 25/100\n",
      "7s - loss: 1.2698 - acc: 0.5481 - val_loss: 2.0530 - val_acc: 0.3280\n",
      "Epoch 26/100\n",
      "7s - loss: 1.2811 - acc: 0.5452 - val_loss: 1.8185 - val_acc: 0.3404\n",
      "Epoch 27/100\n",
      "7s - loss: 1.2761 - acc: 0.5468 - val_loss: 1.6586 - val_acc: 0.4173\n",
      "Epoch 28/100\n",
      "7s - loss: 1.2503 - acc: 0.5545 - val_loss: 1.5495 - val_acc: 0.4777\n",
      "Epoch 29/100\n",
      "7s - loss: 1.2433 - acc: 0.5598 - val_loss: 1.6361 - val_acc: 0.4476\n",
      "Epoch 30/100\n",
      "7s - loss: 1.2267 - acc: 0.5647 - val_loss: 1.3909 - val_acc: 0.4993\n",
      "Epoch 31/100\n",
      "7s - loss: 1.2286 - acc: 0.5641 - val_loss: 1.4367 - val_acc: 0.4934\n",
      "Epoch 32/100\n",
      "7s - loss: 1.2433 - acc: 0.5567 - val_loss: 1.6321 - val_acc: 0.4449\n",
      "Epoch 33/100\n",
      "7s - loss: 1.2157 - acc: 0.5688 - val_loss: 1.5585 - val_acc: 0.4664\n",
      "Epoch 34/100\n",
      "7s - loss: 1.2143 - acc: 0.5692 - val_loss: 1.3773 - val_acc: 0.5175\n",
      "Epoch 35/100\n",
      "7s - loss: 1.1942 - acc: 0.5758 - val_loss: 1.4552 - val_acc: 0.4860\n",
      "Epoch 36/100\n",
      "7s - loss: 1.1878 - acc: 0.5786 - val_loss: 1.4977 - val_acc: 0.4664\n",
      "Epoch 37/100\n",
      "7s - loss: 1.1684 - acc: 0.5858 - val_loss: 1.3889 - val_acc: 0.5126\n",
      "Epoch 38/100\n",
      "7s - loss: 1.1536 - acc: 0.5920 - val_loss: 1.5329 - val_acc: 0.4782\n",
      "Epoch 39/100\n",
      "7s - loss: 1.1447 - acc: 0.5953 - val_loss: 1.4444 - val_acc: 0.5011\n",
      "Epoch 40/100\n",
      "7s - loss: 1.1489 - acc: 0.5958 - val_loss: 1.4956 - val_acc: 0.4877\n",
      "Epoch 41/100\n",
      "7s - loss: 1.1446 - acc: 0.5932 - val_loss: 1.6156 - val_acc: 0.4541\n",
      "Epoch 42/100\n",
      "7s - loss: 1.1364 - acc: 0.5969 - val_loss: 1.5246 - val_acc: 0.4840\n",
      "Epoch 43/100\n",
      "7s - loss: 1.1365 - acc: 0.5993 - val_loss: 1.5538 - val_acc: 0.4790\n",
      "Epoch 44/100\n",
      "7s - loss: 1.1324 - acc: 0.6008 - val_loss: 1.4702 - val_acc: 0.5023\n",
      "Epoch 45/100\n",
      "7s - loss: 1.1110 - acc: 0.6078 - val_loss: 1.4255 - val_acc: 0.5100\n",
      "Epoch 46/100\n",
      "7s - loss: 1.1087 - acc: 0.6076 - val_loss: 1.3955 - val_acc: 0.5145\n",
      "Epoch 47/100\n",
      "7s - loss: 1.1117 - acc: 0.6068 - val_loss: 1.4860 - val_acc: 0.5024\n",
      "Epoch 48/100\n",
      "7s - loss: 1.0883 - acc: 0.6168 - val_loss: 1.4272 - val_acc: 0.5022\n",
      "Epoch 49/100\n",
      "7s - loss: 1.0736 - acc: 0.6203 - val_loss: 1.6403 - val_acc: 0.4552\n",
      "Epoch 50/100\n",
      "7s - loss: 1.0741 - acc: 0.6219 - val_loss: 1.5612 - val_acc: 0.4717\n",
      "Epoch 51/100\n",
      "7s - loss: 1.0663 - acc: 0.6221 - val_loss: 1.5045 - val_acc: 0.4730\n",
      "Epoch 52/100\n",
      "7s - loss: 1.0640 - acc: 0.6256 - val_loss: 1.4590 - val_acc: 0.5021\n",
      "Epoch 53/100\n",
      "7s - loss: 1.0488 - acc: 0.6306 - val_loss: 1.5681 - val_acc: 0.4719\n",
      "Epoch 54/100\n",
      "7s - loss: 1.0403 - acc: 0.6329 - val_loss: 1.3614 - val_acc: 0.5317\n",
      "Epoch 55/100\n",
      "7s - loss: 1.0215 - acc: 0.6404 - val_loss: 1.5239 - val_acc: 0.4950\n",
      "Epoch 56/100\n",
      "7s - loss: 1.0180 - acc: 0.6435 - val_loss: 1.4419 - val_acc: 0.5070\n",
      "Epoch 57/100\n",
      "7s - loss: 1.0087 - acc: 0.6459 - val_loss: 1.4553 - val_acc: 0.5177\n",
      "Epoch 58/100\n",
      "7s - loss: 1.0016 - acc: 0.6488 - val_loss: 1.4981 - val_acc: 0.5086\n",
      "Epoch 59/100\n",
      "7s - loss: 0.9782 - acc: 0.6575 - val_loss: 1.5712 - val_acc: 0.4700\n",
      "Epoch 60/100\n",
      "7s - loss: 0.9867 - acc: 0.6550 - val_loss: 1.4373 - val_acc: 0.5323\n",
      "Epoch 61/100\n",
      "7s - loss: 0.9636 - acc: 0.6627 - val_loss: 1.4884 - val_acc: 0.5176\n",
      "Epoch 62/100\n",
      "7s - loss: 0.9585 - acc: 0.6641 - val_loss: 1.4159 - val_acc: 0.5322\n",
      "Epoch 63/100\n",
      "7s - loss: 0.9741 - acc: 0.6576 - val_loss: 1.5143 - val_acc: 0.4864\n",
      "Epoch 64/100\n",
      "7s - loss: 1.0251 - acc: 0.6380 - val_loss: 1.9196 - val_acc: 0.3999\n",
      "Epoch 65/100\n",
      "7s - loss: 0.9905 - acc: 0.6513 - val_loss: 1.4765 - val_acc: 0.5074\n",
      "Epoch 66/100\n",
      "7s - loss: 0.9753 - acc: 0.6570 - val_loss: 1.4491 - val_acc: 0.5255\n",
      "Epoch 67/100\n",
      "7s - loss: 0.9513 - acc: 0.6667 - val_loss: 1.5026 - val_acc: 0.5201\n",
      "Epoch 68/100\n",
      "7s - loss: 0.9382 - acc: 0.6720 - val_loss: 1.4101 - val_acc: 0.5302\n",
      "Epoch 69/100\n",
      "7s - loss: 0.9222 - acc: 0.6768 - val_loss: 1.4066 - val_acc: 0.5385\n",
      "Epoch 70/100\n",
      "7s - loss: 0.9068 - acc: 0.6835 - val_loss: 1.4622 - val_acc: 0.5230\n",
      "Epoch 71/100\n",
      "7s - loss: 0.9034 - acc: 0.6843 - val_loss: 1.5169 - val_acc: 0.5290\n",
      "Epoch 72/100\n",
      "7s - loss: 0.8898 - acc: 0.6900 - val_loss: 1.6874 - val_acc: 0.4638\n",
      "Epoch 73/100\n",
      "7s - loss: 0.8828 - acc: 0.6926 - val_loss: 1.4800 - val_acc: 0.5244\n",
      "Epoch 74/100\n",
      "7s - loss: 0.8736 - acc: 0.6942 - val_loss: 1.4933 - val_acc: 0.5132\n",
      "Epoch 75/100\n",
      "7s - loss: 0.8704 - acc: 0.6962 - val_loss: 1.3856 - val_acc: 0.5473\n",
      "Epoch 76/100\n",
      "7s - loss: 0.8569 - acc: 0.7010 - val_loss: 1.6039 - val_acc: 0.5090\n",
      "Epoch 77/100\n",
      "7s - loss: 0.8492 - acc: 0.7034 - val_loss: 1.5411 - val_acc: 0.5219\n",
      "Epoch 78/100\n",
      "7s - loss: 0.8360 - acc: 0.7078 - val_loss: 1.4359 - val_acc: 0.5332\n",
      "Epoch 79/100\n",
      "7s - loss: 0.8306 - acc: 0.7097 - val_loss: 1.5297 - val_acc: 0.5321\n",
      "Epoch 80/100\n",
      "7s - loss: 0.8256 - acc: 0.7120 - val_loss: 1.5538 - val_acc: 0.5293\n",
      "Epoch 81/100\n",
      "7s - loss: 0.8216 - acc: 0.7154 - val_loss: 1.4559 - val_acc: 0.5361\n",
      "Epoch 82/100\n",
      "7s - loss: 0.8124 - acc: 0.7150 - val_loss: 1.5519 - val_acc: 0.5281\n",
      "Epoch 83/100\n",
      "7s - loss: 0.8015 - acc: 0.7223 - val_loss: 1.6099 - val_acc: 0.5169\n",
      "Epoch 84/100\n",
      "7s - loss: 0.7949 - acc: 0.7236 - val_loss: 1.8045 - val_acc: 0.4757\n",
      "Epoch 85/100\n",
      "7s - loss: 0.7884 - acc: 0.7273 - val_loss: 1.5578 - val_acc: 0.5262\n",
      "Epoch 86/100\n",
      "7s - loss: 0.7824 - acc: 0.7263 - val_loss: 1.5646 - val_acc: 0.5342\n",
      "Epoch 87/100\n",
      "7s - loss: 0.7896 - acc: 0.7260 - val_loss: 1.5209 - val_acc: 0.5330\n",
      "Epoch 88/100\n",
      "7s - loss: 0.7690 - acc: 0.7336 - val_loss: 1.6041 - val_acc: 0.5107\n",
      "Epoch 89/100\n",
      "7s - loss: 0.7562 - acc: 0.7369 - val_loss: 1.6001 - val_acc: 0.5233\n",
      "Epoch 90/100\n",
      "7s - loss: 0.7488 - acc: 0.7392 - val_loss: 1.6780 - val_acc: 0.5153\n",
      "Epoch 91/100\n",
      "7s - loss: 0.7494 - acc: 0.7404 - val_loss: 1.6884 - val_acc: 0.5001\n",
      "Epoch 92/100\n",
      "7s - loss: 0.7355 - acc: 0.7438 - val_loss: 1.7718 - val_acc: 0.4956\n",
      "Epoch 93/100\n",
      "7s - loss: 0.7350 - acc: 0.7434 - val_loss: 1.5533 - val_acc: 0.5281\n",
      "Epoch 94/100\n",
      "7s - loss: 0.7276 - acc: 0.7484 - val_loss: 1.6742 - val_acc: 0.5052\n",
      "Epoch 95/100\n",
      "7s - loss: 0.7173 - acc: 0.7512 - val_loss: 1.7562 - val_acc: 0.5192\n",
      "Epoch 96/100\n",
      "7s - loss: 0.7062 - acc: 0.7552 - val_loss: 1.6518 - val_acc: 0.5206\n",
      "Epoch 97/100\n",
      "7s - loss: 0.7171 - acc: 0.7487 - val_loss: 1.5971 - val_acc: 0.5321\n",
      "Epoch 98/100\n",
      "7s - loss: 0.7119 - acc: 0.7538 - val_loss: 1.5642 - val_acc: 0.5272\n",
      "Epoch 99/100\n",
      "7s - loss: 0.6991 - acc: 0.7576 - val_loss: 1.5319 - val_acc: 0.5368\n",
      "Epoch 100/100\n",
      "7s - loss: 0.6848 - acc: 0.7630 - val_loss: 1.7503 - val_acc: 0.5006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7596f0cd90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/cifar10_5_layers_BatchNormalization')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe score of this model is %50.0600 \n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "final_score = scores[1]*100\n",
    "print ('The score of this model is %%%.4f ' % final_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Digits Data set\n",
    "# try to use the tricks with Digits DataSet \n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "#load Digits dataset \n",
    "data=load_digits()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "\n",
    "# Train / Test \n",
    "X_train = X[:1500]\n",
    "Y_train = Y[:1500]\n",
    "X_test = X[1500:]\n",
    "Y_test = Y[1500:]\n",
    "\n",
    "# Normalize (Divide by 255)\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one_hot_encoding the training output \n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Deep learning with 2 hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=64, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.2943 - acc: 0.1487 - val_loss: 2.2813 - val_acc: 0.3098\n",
      "Epoch 2/100\n",
      "0s - loss: 2.2645 - acc: 0.5013 - val_loss: 2.2414 - val_acc: 0.5522\n",
      "Epoch 3/100\n",
      "0s - loss: 2.2091 - acc: 0.5533 - val_loss: 2.1647 - val_acc: 0.5690\n",
      "Epoch 4/100\n",
      "0s - loss: 2.1043 - acc: 0.6767 - val_loss: 2.0282 - val_acc: 0.6128\n",
      "Epoch 5/100\n",
      "0s - loss: 1.9254 - acc: 0.7593 - val_loss: 1.8089 - val_acc: 0.7104\n",
      "Epoch 6/100\n",
      "0s - loss: 1.6586 - acc: 0.7687 - val_loss: 1.5217 - val_acc: 0.7441\n",
      "Epoch 7/100\n",
      "0s - loss: 1.3389 - acc: 0.8040 - val_loss: 1.2222 - val_acc: 0.7576\n",
      "Epoch 8/100\n",
      "0s - loss: 1.0298 - acc: 0.8373 - val_loss: 1.0259 - val_acc: 0.7609\n",
      "Epoch 9/100\n",
      "0s - loss: 0.7898 - acc: 0.8607 - val_loss: 0.8229 - val_acc: 0.7980\n",
      "Epoch 10/100\n",
      "0s - loss: 0.6218 - acc: 0.8773 - val_loss: 0.7201 - val_acc: 0.8081\n",
      "Epoch 11/100\n",
      "0s - loss: 0.5141 - acc: 0.8840 - val_loss: 0.6794 - val_acc: 0.8215\n",
      "Epoch 12/100\n",
      "0s - loss: 0.4456 - acc: 0.8873 - val_loss: 0.5975 - val_acc: 0.8418\n",
      "Epoch 13/100\n",
      "0s - loss: 0.3961 - acc: 0.8987 - val_loss: 0.5921 - val_acc: 0.8316\n",
      "Epoch 14/100\n",
      "0s - loss: 0.3532 - acc: 0.9093 - val_loss: 0.5447 - val_acc: 0.8350\n",
      "Epoch 15/100\n",
      "0s - loss: 0.3151 - acc: 0.9193 - val_loss: 0.5343 - val_acc: 0.8519\n",
      "Epoch 16/100\n",
      "0s - loss: 0.2994 - acc: 0.9200 - val_loss: 0.4773 - val_acc: 0.8687\n",
      "Epoch 17/100\n",
      "0s - loss: 0.2687 - acc: 0.9287 - val_loss: 0.4947 - val_acc: 0.8586\n",
      "Epoch 18/100\n",
      "0s - loss: 0.2456 - acc: 0.9333 - val_loss: 0.4558 - val_acc: 0.8687\n",
      "Epoch 19/100\n",
      "0s - loss: 0.2293 - acc: 0.9393 - val_loss: 0.4439 - val_acc: 0.8754\n",
      "Epoch 20/100\n",
      "0s - loss: 0.2143 - acc: 0.9380 - val_loss: 0.4317 - val_acc: 0.8788\n",
      "Epoch 21/100\n",
      "0s - loss: 0.2045 - acc: 0.9440 - val_loss: 0.4097 - val_acc: 0.8956\n",
      "Epoch 22/100\n",
      "0s - loss: 0.1854 - acc: 0.9480 - val_loss: 0.4431 - val_acc: 0.8620\n",
      "Epoch 23/100\n",
      "0s - loss: 0.1770 - acc: 0.9500 - val_loss: 0.3891 - val_acc: 0.8956\n",
      "Epoch 24/100\n",
      "0s - loss: 0.1662 - acc: 0.9580 - val_loss: 0.4226 - val_acc: 0.8721\n",
      "Epoch 25/100\n",
      "0s - loss: 0.1544 - acc: 0.9560 - val_loss: 0.3750 - val_acc: 0.8956\n",
      "Epoch 26/100\n",
      "0s - loss: 0.1444 - acc: 0.9620 - val_loss: 0.3792 - val_acc: 0.8990\n",
      "Epoch 27/100\n",
      "0s - loss: 0.1398 - acc: 0.9647 - val_loss: 0.4101 - val_acc: 0.8687\n",
      "Epoch 28/100\n",
      "0s - loss: 0.1346 - acc: 0.9640 - val_loss: 0.3591 - val_acc: 0.8990\n",
      "Epoch 29/100\n",
      "0s - loss: 0.1263 - acc: 0.9667 - val_loss: 0.3513 - val_acc: 0.8990\n",
      "Epoch 30/100\n",
      "0s - loss: 0.1200 - acc: 0.9693 - val_loss: 0.3871 - val_acc: 0.8855\n",
      "Epoch 31/100\n",
      "0s - loss: 0.1139 - acc: 0.9720 - val_loss: 0.3450 - val_acc: 0.9091\n",
      "Epoch 32/100\n",
      "0s - loss: 0.1089 - acc: 0.9713 - val_loss: 0.3476 - val_acc: 0.8990\n",
      "Epoch 33/100\n",
      "0s - loss: 0.1062 - acc: 0.9720 - val_loss: 0.3689 - val_acc: 0.8889\n",
      "Epoch 34/100\n",
      "0s - loss: 0.1036 - acc: 0.9720 - val_loss: 0.3359 - val_acc: 0.9057\n",
      "Epoch 35/100\n",
      "0s - loss: 0.0961 - acc: 0.9740 - val_loss: 0.3586 - val_acc: 0.8923\n",
      "Epoch 36/100\n",
      "0s - loss: 0.0924 - acc: 0.9733 - val_loss: 0.3348 - val_acc: 0.9057\n",
      "Epoch 37/100\n",
      "0s - loss: 0.0887 - acc: 0.9773 - val_loss: 0.3512 - val_acc: 0.8990\n",
      "Epoch 38/100\n",
      "0s - loss: 0.0834 - acc: 0.9773 - val_loss: 0.3380 - val_acc: 0.8990\n",
      "Epoch 39/100\n",
      "0s - loss: 0.0797 - acc: 0.9827 - val_loss: 0.3396 - val_acc: 0.9091\n",
      "Epoch 40/100\n",
      "0s - loss: 0.0792 - acc: 0.9807 - val_loss: 0.3579 - val_acc: 0.8923\n",
      "Epoch 41/100\n",
      "0s - loss: 0.0780 - acc: 0.9793 - val_loss: 0.3338 - val_acc: 0.8956\n",
      "Epoch 42/100\n",
      "0s - loss: 0.0748 - acc: 0.9820 - val_loss: 0.3489 - val_acc: 0.9158\n",
      "Epoch 43/100\n",
      "0s - loss: 0.0715 - acc: 0.9827 - val_loss: 0.3292 - val_acc: 0.9057\n",
      "Epoch 44/100\n",
      "0s - loss: 0.0670 - acc: 0.9833 - val_loss: 0.3452 - val_acc: 0.8990\n",
      "Epoch 45/100\n",
      "0s - loss: 0.0645 - acc: 0.9860 - val_loss: 0.3223 - val_acc: 0.9125\n",
      "Epoch 46/100\n",
      "0s - loss: 0.0637 - acc: 0.9840 - val_loss: 0.3384 - val_acc: 0.8990\n",
      "Epoch 47/100\n",
      "0s - loss: 0.0607 - acc: 0.9860 - val_loss: 0.3250 - val_acc: 0.9091\n",
      "Epoch 48/100\n",
      "0s - loss: 0.0584 - acc: 0.9887 - val_loss: 0.3456 - val_acc: 0.8956\n",
      "Epoch 49/100\n",
      "0s - loss: 0.0587 - acc: 0.9867 - val_loss: 0.3265 - val_acc: 0.9024\n",
      "Epoch 50/100\n",
      "0s - loss: 0.0568 - acc: 0.9887 - val_loss: 0.3437 - val_acc: 0.9024\n",
      "Epoch 51/100\n",
      "0s - loss: 0.0554 - acc: 0.9873 - val_loss: 0.3272 - val_acc: 0.9024\n",
      "Epoch 52/100\n",
      "0s - loss: 0.0534 - acc: 0.9860 - val_loss: 0.3301 - val_acc: 0.9024\n",
      "Epoch 53/100\n",
      "0s - loss: 0.0512 - acc: 0.9880 - val_loss: 0.3257 - val_acc: 0.9024\n",
      "Epoch 54/100\n",
      "0s - loss: 0.0503 - acc: 0.9880 - val_loss: 0.3283 - val_acc: 0.9057\n",
      "Epoch 55/100\n",
      "0s - loss: 0.0473 - acc: 0.9913 - val_loss: 0.3257 - val_acc: 0.9158\n",
      "Epoch 56/100\n",
      "0s - loss: 0.0474 - acc: 0.9907 - val_loss: 0.3221 - val_acc: 0.9125\n",
      "Epoch 57/100\n",
      "0s - loss: 0.0461 - acc: 0.9893 - val_loss: 0.3434 - val_acc: 0.9091\n",
      "Epoch 58/100\n",
      "0s - loss: 0.0438 - acc: 0.9907 - val_loss: 0.3319 - val_acc: 0.9125\n",
      "Epoch 59/100\n",
      "0s - loss: 0.0413 - acc: 0.9893 - val_loss: 0.3377 - val_acc: 0.9057\n",
      "Epoch 60/100\n",
      "0s - loss: 0.0418 - acc: 0.9907 - val_loss: 0.3334 - val_acc: 0.9091\n",
      "Epoch 61/100\n",
      "0s - loss: 0.0403 - acc: 0.9927 - val_loss: 0.3397 - val_acc: 0.9057\n",
      "Epoch 62/100\n",
      "0s - loss: 0.0374 - acc: 0.9933 - val_loss: 0.3443 - val_acc: 0.9024\n",
      "Epoch 63/100\n",
      "0s - loss: 0.0369 - acc: 0.9907 - val_loss: 0.3331 - val_acc: 0.9057\n",
      "Epoch 64/100\n",
      "0s - loss: 0.0396 - acc: 0.9940 - val_loss: 0.3673 - val_acc: 0.9057\n",
      "Epoch 65/100\n",
      "0s - loss: 0.0360 - acc: 0.9920 - val_loss: 0.3282 - val_acc: 0.9057\n",
      "Epoch 66/100\n",
      "0s - loss: 0.0362 - acc: 0.9927 - val_loss: 0.3490 - val_acc: 0.9091\n",
      "Epoch 67/100\n",
      "0s - loss: 0.0320 - acc: 0.9940 - val_loss: 0.3316 - val_acc: 0.9125\n",
      "Epoch 68/100\n",
      "0s - loss: 0.0309 - acc: 0.9960 - val_loss: 0.3531 - val_acc: 0.9024\n",
      "Epoch 69/100\n",
      "0s - loss: 0.0302 - acc: 0.9960 - val_loss: 0.3415 - val_acc: 0.9125\n",
      "Epoch 70/100\n",
      "0s - loss: 0.0286 - acc: 0.9953 - val_loss: 0.3461 - val_acc: 0.9125\n",
      "Epoch 71/100\n",
      "0s - loss: 0.0277 - acc: 0.9953 - val_loss: 0.3460 - val_acc: 0.9091\n",
      "Epoch 72/100\n",
      "0s - loss: 0.0264 - acc: 0.9973 - val_loss: 0.3499 - val_acc: 0.9125\n",
      "Epoch 73/100\n",
      "0s - loss: 0.0260 - acc: 0.9967 - val_loss: 0.3440 - val_acc: 0.9158\n",
      "Epoch 74/100\n",
      "0s - loss: 0.0272 - acc: 0.9940 - val_loss: 0.3380 - val_acc: 0.9125\n",
      "Epoch 75/100\n",
      "0s - loss: 0.0265 - acc: 0.9980 - val_loss: 0.3597 - val_acc: 0.9125\n",
      "Epoch 76/100\n",
      "0s - loss: 0.0254 - acc: 0.9967 - val_loss: 0.3449 - val_acc: 0.9125\n",
      "Epoch 77/100\n",
      "0s - loss: 0.0251 - acc: 0.9987 - val_loss: 0.3838 - val_acc: 0.9057\n",
      "Epoch 78/100\n",
      "0s - loss: 0.0245 - acc: 0.9953 - val_loss: 0.3408 - val_acc: 0.9091\n",
      "Epoch 79/100\n",
      "0s - loss: 0.0230 - acc: 0.9980 - val_loss: 0.3799 - val_acc: 0.9024\n",
      "Epoch 80/100\n",
      "0s - loss: 0.0243 - acc: 0.9960 - val_loss: 0.3466 - val_acc: 0.9091\n",
      "Epoch 81/100\n",
      "0s - loss: 0.0219 - acc: 0.9953 - val_loss: 0.3496 - val_acc: 0.9158\n",
      "Epoch 82/100\n",
      "0s - loss: 0.0206 - acc: 0.9987 - val_loss: 0.3519 - val_acc: 0.9158\n",
      "Epoch 83/100\n",
      "0s - loss: 0.0207 - acc: 0.9980 - val_loss: 0.3640 - val_acc: 0.9158\n",
      "Epoch 84/100\n",
      "0s - loss: 0.0208 - acc: 0.9980 - val_loss: 0.3559 - val_acc: 0.9158\n",
      "Epoch 85/100\n",
      "0s - loss: 0.0188 - acc: 0.9993 - val_loss: 0.3573 - val_acc: 0.9158\n",
      "Epoch 86/100\n",
      "0s - loss: 0.0188 - acc: 0.9980 - val_loss: 0.3663 - val_acc: 0.9125\n",
      "Epoch 87/100\n",
      "0s - loss: 0.0191 - acc: 0.9973 - val_loss: 0.3604 - val_acc: 0.9091\n",
      "Epoch 88/100\n",
      "0s - loss: 0.0186 - acc: 0.9987 - val_loss: 0.3670 - val_acc: 0.9125\n",
      "Epoch 89/100\n",
      "0s - loss: 0.0176 - acc: 0.9980 - val_loss: 0.3650 - val_acc: 0.9158\n",
      "Epoch 90/100\n",
      "0s - loss: 0.0160 - acc: 0.9993 - val_loss: 0.3685 - val_acc: 0.9158\n",
      "Epoch 91/100\n",
      "0s - loss: 0.0163 - acc: 0.9993 - val_loss: 0.3665 - val_acc: 0.9158\n",
      "Epoch 92/100\n",
      "0s - loss: 0.0153 - acc: 0.9993 - val_loss: 0.3707 - val_acc: 0.9192\n",
      "Epoch 93/100\n",
      "0s - loss: 0.0150 - acc: 0.9987 - val_loss: 0.3717 - val_acc: 0.9192\n",
      "Epoch 94/100\n",
      "0s - loss: 0.0147 - acc: 0.9993 - val_loss: 0.3677 - val_acc: 0.9158\n",
      "Epoch 95/100\n",
      "0s - loss: 0.0148 - acc: 0.9987 - val_loss: 0.3670 - val_acc: 0.9158\n",
      "Epoch 96/100\n",
      "0s - loss: 0.0146 - acc: 1.0000 - val_loss: 0.3805 - val_acc: 0.9125\n",
      "Epoch 97/100\n",
      "0s - loss: 0.0151 - acc: 0.9993 - val_loss: 0.3845 - val_acc: 0.9125\n",
      "Epoch 98/100\n",
      "0s - loss: 0.0129 - acc: 1.0000 - val_loss: 0.3745 - val_acc: 0.9226\n",
      "Epoch 99/100\n",
      "0s - loss: 0.0130 - acc: 0.9993 - val_loss: 0.3746 - val_acc: 0.9158\n",
      "Epoch 100/100\n",
      "0s - loss: 0.0125 - acc: 1.0000 - val_loss: 0.3804 - val_acc: 0.9158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f754e3798d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/Digits_2layers_PReLU')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep learning with 2 hidden layers and some tricks\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=64, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.1481 - acc: 0.3460 - val_loss: 2.2896 - val_acc: 0.0909\n",
      "Epoch 2/100\n",
      "0s - loss: 0.7330 - acc: 0.7580 - val_loss: 2.2806 - val_acc: 0.0909\n",
      "Epoch 3/100\n",
      "0s - loss: 0.4505 - acc: 0.8620 - val_loss: 2.2779 - val_acc: 0.1044\n",
      "Epoch 4/100\n",
      "0s - loss: 0.3130 - acc: 0.9107 - val_loss: 2.2826 - val_acc: 0.0943\n",
      "Epoch 5/100\n",
      "0s - loss: 0.2842 - acc: 0.9087 - val_loss: 2.2897 - val_acc: 0.0943\n",
      "Epoch 6/100\n",
      "0s - loss: 0.1979 - acc: 0.9393 - val_loss: 2.2965 - val_acc: 0.0943\n",
      "Epoch 7/100\n",
      "0s - loss: 0.2010 - acc: 0.9387 - val_loss: 2.3015 - val_acc: 0.0943\n",
      "Epoch 8/100\n",
      "0s - loss: 0.1708 - acc: 0.9433 - val_loss: 2.3066 - val_acc: 0.0943\n",
      "Epoch 9/100\n",
      "0s - loss: 0.1583 - acc: 0.9527 - val_loss: 2.3211 - val_acc: 0.0943\n",
      "Epoch 10/100\n",
      "0s - loss: 0.1543 - acc: 0.9560 - val_loss: 2.3348 - val_acc: 0.0943\n",
      "Epoch 11/100\n",
      "0s - loss: 0.1321 - acc: 0.9627 - val_loss: 2.3380 - val_acc: 0.0943\n",
      "Epoch 12/100\n",
      "0s - loss: 0.1291 - acc: 0.9520 - val_loss: 2.3469 - val_acc: 0.0943\n",
      "Epoch 13/100\n",
      "0s - loss: 0.1199 - acc: 0.9633 - val_loss: 2.3496 - val_acc: 0.0943\n",
      "Epoch 14/100\n",
      "0s - loss: 0.1105 - acc: 0.9647 - val_loss: 2.3749 - val_acc: 0.0943\n",
      "Epoch 15/100\n",
      "0s - loss: 0.0986 - acc: 0.9713 - val_loss: 2.3965 - val_acc: 0.0943\n",
      "Epoch 16/100\n",
      "0s - loss: 0.1077 - acc: 0.9693 - val_loss: 2.4180 - val_acc: 0.0943\n",
      "Epoch 17/100\n",
      "0s - loss: 0.0978 - acc: 0.9713 - val_loss: 2.4284 - val_acc: 0.0943\n",
      "Epoch 18/100\n",
      "0s - loss: 0.0895 - acc: 0.9687 - val_loss: 2.4517 - val_acc: 0.0943\n",
      "Epoch 19/100\n",
      "0s - loss: 0.0779 - acc: 0.9733 - val_loss: 2.4847 - val_acc: 0.0943\n",
      "Epoch 20/100\n",
      "0s - loss: 0.0819 - acc: 0.9747 - val_loss: 2.5208 - val_acc: 0.0943\n",
      "Epoch 21/100\n",
      "0s - loss: 0.0821 - acc: 0.9753 - val_loss: 2.5698 - val_acc: 0.0943\n",
      "Epoch 22/100\n",
      "0s - loss: 0.0662 - acc: 0.9800 - val_loss: 2.5936 - val_acc: 0.0943\n",
      "Epoch 23/100\n",
      "0s - loss: 0.0601 - acc: 0.9833 - val_loss: 2.5677 - val_acc: 0.0943\n",
      "Epoch 24/100\n",
      "0s - loss: 0.0533 - acc: 0.9860 - val_loss: 2.5804 - val_acc: 0.0943\n",
      "Epoch 25/100\n",
      "0s - loss: 0.0546 - acc: 0.9813 - val_loss: 2.6201 - val_acc: 0.0943\n",
      "Epoch 26/100\n",
      "0s - loss: 0.0503 - acc: 0.9840 - val_loss: 2.6353 - val_acc: 0.0943\n",
      "Epoch 27/100\n",
      "0s - loss: 0.0619 - acc: 0.9833 - val_loss: 2.6009 - val_acc: 0.0943\n",
      "Epoch 28/100\n",
      "0s - loss: 0.0575 - acc: 0.9847 - val_loss: 2.6214 - val_acc: 0.0943\n",
      "Epoch 29/100\n",
      "0s - loss: 0.0543 - acc: 0.9827 - val_loss: 2.5991 - val_acc: 0.0943\n",
      "Epoch 30/100\n",
      "0s - loss: 0.0477 - acc: 0.9833 - val_loss: 2.5813 - val_acc: 0.0943\n",
      "Epoch 31/100\n",
      "0s - loss: 0.0491 - acc: 0.9873 - val_loss: 2.6465 - val_acc: 0.0943\n",
      "Epoch 32/100\n",
      "0s - loss: 0.0505 - acc: 0.9853 - val_loss: 2.6387 - val_acc: 0.0943\n",
      "Epoch 33/100\n",
      "0s - loss: 0.0368 - acc: 0.9907 - val_loss: 2.6271 - val_acc: 0.0943\n",
      "Epoch 34/100\n",
      "0s - loss: 0.0485 - acc: 0.9880 - val_loss: 2.6739 - val_acc: 0.0943\n",
      "Epoch 35/100\n",
      "0s - loss: 0.0385 - acc: 0.9887 - val_loss: 2.6771 - val_acc: 0.0943\n",
      "Epoch 36/100\n",
      "0s - loss: 0.0476 - acc: 0.9827 - val_loss: 2.6335 - val_acc: 0.0943\n",
      "Epoch 37/100\n",
      "0s - loss: 0.0331 - acc: 0.9927 - val_loss: 2.7524 - val_acc: 0.0943\n",
      "Epoch 38/100\n",
      "0s - loss: 0.0432 - acc: 0.9860 - val_loss: 2.7325 - val_acc: 0.0943\n",
      "Epoch 39/100\n",
      "0s - loss: 0.0377 - acc: 0.9887 - val_loss: 2.7032 - val_acc: 0.0943\n",
      "Epoch 40/100\n",
      "0s - loss: 0.0377 - acc: 0.9913 - val_loss: 2.7425 - val_acc: 0.0943\n",
      "Epoch 41/100\n",
      "0s - loss: 0.0358 - acc: 0.9907 - val_loss: 2.7283 - val_acc: 0.0943\n",
      "Epoch 42/100\n",
      "0s - loss: 0.0345 - acc: 0.9880 - val_loss: 2.6250 - val_acc: 0.0943\n",
      "Epoch 43/100\n",
      "0s - loss: 0.0355 - acc: 0.9900 - val_loss: 2.5176 - val_acc: 0.0976\n",
      "Epoch 44/100\n",
      "0s - loss: 0.0328 - acc: 0.9880 - val_loss: 2.4611 - val_acc: 0.0976\n",
      "Epoch 45/100\n",
      "0s - loss: 0.0314 - acc: 0.9920 - val_loss: 2.4719 - val_acc: 0.1077\n",
      "Epoch 46/100\n",
      "0s - loss: 0.0218 - acc: 0.9953 - val_loss: 2.4679 - val_acc: 0.1145\n",
      "Epoch 47/100\n",
      "0s - loss: 0.0296 - acc: 0.9893 - val_loss: 2.4687 - val_acc: 0.1178\n",
      "Epoch 48/100\n",
      "0s - loss: 0.0274 - acc: 0.9933 - val_loss: 2.3866 - val_acc: 0.1279\n",
      "Epoch 49/100\n",
      "0s - loss: 0.0248 - acc: 0.9933 - val_loss: 2.2761 - val_acc: 0.1549\n",
      "Epoch 50/100\n",
      "0s - loss: 0.0269 - acc: 0.9920 - val_loss: 2.0839 - val_acc: 0.2020\n",
      "Epoch 51/100\n",
      "0s - loss: 0.0215 - acc: 0.9947 - val_loss: 1.9419 - val_acc: 0.2593\n",
      "Epoch 52/100\n",
      "0s - loss: 0.0276 - acc: 0.9920 - val_loss: 1.9209 - val_acc: 0.2963\n",
      "Epoch 53/100\n",
      "0s - loss: 0.0204 - acc: 0.9960 - val_loss: 1.8937 - val_acc: 0.3300\n",
      "Epoch 54/100\n",
      "0s - loss: 0.0267 - acc: 0.9927 - val_loss: 1.7645 - val_acc: 0.3973\n",
      "Epoch 55/100\n",
      "0s - loss: 0.0185 - acc: 0.9960 - val_loss: 1.6382 - val_acc: 0.4343\n",
      "Epoch 56/100\n",
      "0s - loss: 0.0216 - acc: 0.9953 - val_loss: 1.4279 - val_acc: 0.4848\n",
      "Epoch 57/100\n",
      "0s - loss: 0.0234 - acc: 0.9920 - val_loss: 1.2889 - val_acc: 0.5253\n",
      "Epoch 58/100\n",
      "0s - loss: 0.0228 - acc: 0.9913 - val_loss: 1.2518 - val_acc: 0.5657\n",
      "Epoch 59/100\n",
      "0s - loss: 0.0163 - acc: 0.9960 - val_loss: 1.1803 - val_acc: 0.5960\n",
      "Epoch 60/100\n",
      "0s - loss: 0.0209 - acc: 0.9947 - val_loss: 1.0376 - val_acc: 0.6330\n",
      "Epoch 61/100\n",
      "0s - loss: 0.0172 - acc: 0.9960 - val_loss: 0.9223 - val_acc: 0.6835\n",
      "Epoch 62/100\n",
      "0s - loss: 0.0219 - acc: 0.9953 - val_loss: 0.8235 - val_acc: 0.7306\n",
      "Epoch 63/100\n",
      "0s - loss: 0.0208 - acc: 0.9947 - val_loss: 0.7221 - val_acc: 0.7475\n",
      "Epoch 64/100\n",
      "0s - loss: 0.0196 - acc: 0.9940 - val_loss: 0.7506 - val_acc: 0.7542\n",
      "Epoch 65/100\n",
      "0s - loss: 0.0149 - acc: 0.9960 - val_loss: 0.8466 - val_acc: 0.7340\n",
      "Epoch 66/100\n",
      "0s - loss: 0.0198 - acc: 0.9940 - val_loss: 0.7132 - val_acc: 0.7710\n",
      "Epoch 67/100\n",
      "0s - loss: 0.0189 - acc: 0.9953 - val_loss: 0.6243 - val_acc: 0.7879\n",
      "Epoch 68/100\n",
      "0s - loss: 0.0159 - acc: 0.9973 - val_loss: 0.5159 - val_acc: 0.8350\n",
      "Epoch 69/100\n",
      "0s - loss: 0.0151 - acc: 0.9960 - val_loss: 0.4636 - val_acc: 0.8552\n",
      "Epoch 70/100\n",
      "0s - loss: 0.0167 - acc: 0.9973 - val_loss: 0.4830 - val_acc: 0.8485\n",
      "Epoch 71/100\n",
      "0s - loss: 0.0160 - acc: 0.9960 - val_loss: 0.4775 - val_acc: 0.8451\n",
      "Epoch 72/100\n",
      "0s - loss: 0.0151 - acc: 0.9960 - val_loss: 0.4513 - val_acc: 0.8519\n",
      "Epoch 73/100\n",
      "0s - loss: 0.0175 - acc: 0.9967 - val_loss: 0.4407 - val_acc: 0.8653\n",
      "Epoch 74/100\n",
      "0s - loss: 0.0167 - acc: 0.9960 - val_loss: 0.3883 - val_acc: 0.8956\n",
      "Epoch 75/100\n",
      "0s - loss: 0.0177 - acc: 0.9933 - val_loss: 0.3398 - val_acc: 0.9158\n",
      "Epoch 76/100\n",
      "0s - loss: 0.0216 - acc: 0.9947 - val_loss: 0.3272 - val_acc: 0.9259\n",
      "Epoch 77/100\n",
      "0s - loss: 0.0159 - acc: 0.9967 - val_loss: 0.2979 - val_acc: 0.9327\n",
      "Epoch 78/100\n",
      "0s - loss: 0.0123 - acc: 0.9987 - val_loss: 0.2803 - val_acc: 0.9394\n",
      "Epoch 79/100\n",
      "0s - loss: 0.0127 - acc: 0.9973 - val_loss: 0.2977 - val_acc: 0.9226\n",
      "Epoch 80/100\n",
      "0s - loss: 0.0117 - acc: 0.9987 - val_loss: 0.2997 - val_acc: 0.9327\n",
      "Epoch 81/100\n",
      "0s - loss: 0.0121 - acc: 0.9967 - val_loss: 0.3081 - val_acc: 0.9360\n",
      "Epoch 82/100\n",
      "0s - loss: 0.0126 - acc: 0.9967 - val_loss: 0.2979 - val_acc: 0.9394\n",
      "Epoch 83/100\n",
      "0s - loss: 0.0132 - acc: 0.9980 - val_loss: 0.2773 - val_acc: 0.9428\n",
      "Epoch 84/100\n",
      "0s - loss: 0.0156 - acc: 0.9967 - val_loss: 0.2788 - val_acc: 0.9428\n",
      "Epoch 85/100\n",
      "0s - loss: 0.0125 - acc: 0.9967 - val_loss: 0.2734 - val_acc: 0.9394\n",
      "Epoch 86/100\n",
      "0s - loss: 0.0108 - acc: 0.9980 - val_loss: 0.2715 - val_acc: 0.9461\n",
      "Epoch 87/100\n",
      "0s - loss: 0.0102 - acc: 0.9973 - val_loss: 0.2796 - val_acc: 0.9461\n",
      "Epoch 88/100\n",
      "0s - loss: 0.0108 - acc: 0.9973 - val_loss: 0.2860 - val_acc: 0.9394\n",
      "Epoch 89/100\n",
      "0s - loss: 0.0163 - acc: 0.9953 - val_loss: 0.2818 - val_acc: 0.9428\n",
      "Epoch 90/100\n",
      "0s - loss: 0.0138 - acc: 0.9960 - val_loss: 0.2792 - val_acc: 0.9394\n",
      "Epoch 91/100\n",
      "0s - loss: 0.0106 - acc: 0.9960 - val_loss: 0.2921 - val_acc: 0.9360\n",
      "Epoch 92/100\n",
      "0s - loss: 0.0197 - acc: 0.9933 - val_loss: 0.3121 - val_acc: 0.9259\n",
      "Epoch 93/100\n",
      "0s - loss: 0.0181 - acc: 0.9953 - val_loss: 0.3170 - val_acc: 0.9360\n",
      "Epoch 94/100\n",
      "0s - loss: 0.0154 - acc: 0.9940 - val_loss: 0.3330 - val_acc: 0.9360\n",
      "Epoch 95/100\n",
      "0s - loss: 0.0103 - acc: 0.9973 - val_loss: 0.3399 - val_acc: 0.9327\n",
      "Epoch 96/100\n",
      "0s - loss: 0.0105 - acc: 0.9960 - val_loss: 0.3508 - val_acc: 0.9293\n",
      "Epoch 97/100\n",
      "0s - loss: 0.0096 - acc: 0.9967 - val_loss: 0.3564 - val_acc: 0.9226\n",
      "Epoch 98/100\n",
      "0s - loss: 0.0094 - acc: 0.9987 - val_loss: 0.3447 - val_acc: 0.9192\n",
      "Epoch 99/100\n",
      "0s - loss: 0.0106 - acc: 0.9987 - val_loss: 0.3232 - val_acc: 0.9226\n",
      "Epoch 100/100\n",
      "0s - loss: 0.0089 - acc: 0.9980 - val_loss: 0.3091 - val_acc: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f752b35bb90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/Digits_2layers_Dropout_BatchNormalization')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dropout is the most common way to reduce overfitting.\n",
    "#Here we are adding the same 5 hidden layers (Prelu activation)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=64, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(400, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.1169 - acc: 0.2313 - val_loss: 2.3020 - val_acc: 0.1010\n",
      "Epoch 2/100\n",
      "0s - loss: 1.5127 - acc: 0.5940 - val_loss: 2.3008 - val_acc: 0.2323\n",
      "Epoch 3/100\n",
      "0s - loss: 1.0502 - acc: 0.8160 - val_loss: 2.3000 - val_acc: 0.0943\n",
      "Epoch 4/100\n",
      "0s - loss: 0.7034 - acc: 0.8953 - val_loss: 2.3031 - val_acc: 0.0943\n",
      "Epoch 5/100\n",
      "0s - loss: 0.4774 - acc: 0.9253 - val_loss: 2.3127 - val_acc: 0.0943\n",
      "Epoch 6/100\n",
      "0s - loss: 0.3375 - acc: 0.9480 - val_loss: 2.3322 - val_acc: 0.0943\n",
      "Epoch 7/100\n",
      "0s - loss: 0.2336 - acc: 0.9660 - val_loss: 2.3589 - val_acc: 0.0943\n",
      "Epoch 8/100\n",
      "0s - loss: 0.1811 - acc: 0.9740 - val_loss: 2.3902 - val_acc: 0.0943\n",
      "Epoch 9/100\n",
      "0s - loss: 0.1404 - acc: 0.9807 - val_loss: 2.4367 - val_acc: 0.0943\n",
      "Epoch 10/100\n",
      "0s - loss: 0.1092 - acc: 0.9867 - val_loss: 2.5115 - val_acc: 0.0943\n",
      "Epoch 11/100\n",
      "0s - loss: 0.0953 - acc: 0.9820 - val_loss: 2.6015 - val_acc: 0.0943\n",
      "Epoch 12/100\n",
      "0s - loss: 0.0847 - acc: 0.9887 - val_loss: 2.6915 - val_acc: 0.0943\n",
      "Epoch 13/100\n",
      "0s - loss: 0.0696 - acc: 0.9893 - val_loss: 2.8103 - val_acc: 0.0943\n",
      "Epoch 14/100\n",
      "0s - loss: 0.0573 - acc: 0.9927 - val_loss: 2.8744 - val_acc: 0.0943\n",
      "Epoch 15/100\n",
      "0s - loss: 0.0524 - acc: 0.9920 - val_loss: 2.9525 - val_acc: 0.0943\n",
      "Epoch 16/100\n",
      "0s - loss: 0.0455 - acc: 0.9940 - val_loss: 3.0568 - val_acc: 0.0943\n",
      "Epoch 17/100\n",
      "0s - loss: 0.0402 - acc: 0.9940 - val_loss: 3.2059 - val_acc: 0.0943\n",
      "Epoch 18/100\n",
      "0s - loss: 0.0356 - acc: 0.9953 - val_loss: 3.3800 - val_acc: 0.0943\n",
      "Epoch 19/100\n",
      "0s - loss: 0.0344 - acc: 0.9940 - val_loss: 3.4694 - val_acc: 0.0943\n",
      "Epoch 20/100\n",
      "0s - loss: 0.0315 - acc: 0.9960 - val_loss: 3.5385 - val_acc: 0.0943\n",
      "Epoch 21/100\n",
      "0s - loss: 0.0313 - acc: 0.9973 - val_loss: 3.6610 - val_acc: 0.0943\n",
      "Epoch 22/100\n",
      "0s - loss: 0.0296 - acc: 0.9953 - val_loss: 3.7332 - val_acc: 0.0943\n",
      "Epoch 23/100\n",
      "0s - loss: 0.0242 - acc: 0.9960 - val_loss: 3.8132 - val_acc: 0.0943\n",
      "Epoch 24/100\n",
      "0s - loss: 0.0226 - acc: 0.9967 - val_loss: 4.0235 - val_acc: 0.0943\n",
      "Epoch 25/100\n",
      "0s - loss: 0.0227 - acc: 0.9967 - val_loss: 4.1751 - val_acc: 0.0943\n",
      "Epoch 26/100\n",
      "0s - loss: 0.0208 - acc: 0.9967 - val_loss: 4.2563 - val_acc: 0.0943\n",
      "Epoch 27/100\n",
      "0s - loss: 0.0187 - acc: 0.9980 - val_loss: 4.2758 - val_acc: 0.0943\n",
      "Epoch 28/100\n",
      "0s - loss: 0.0264 - acc: 0.9927 - val_loss: 4.2903 - val_acc: 0.0943\n",
      "Epoch 29/100\n",
      "0s - loss: 0.0193 - acc: 0.9953 - val_loss: 4.4216 - val_acc: 0.0943\n",
      "Epoch 30/100\n",
      "0s - loss: 0.0179 - acc: 0.9980 - val_loss: 4.4933 - val_acc: 0.0943\n",
      "Epoch 31/100\n",
      "0s - loss: 0.0149 - acc: 0.9967 - val_loss: 4.5736 - val_acc: 0.0943\n",
      "Epoch 32/100\n",
      "0s - loss: 0.0171 - acc: 0.9967 - val_loss: 4.5667 - val_acc: 0.0943\n",
      "Epoch 33/100\n",
      "0s - loss: 0.0140 - acc: 0.9980 - val_loss: 4.7380 - val_acc: 0.0943\n",
      "Epoch 34/100\n",
      "0s - loss: 0.0198 - acc: 0.9967 - val_loss: 4.9160 - val_acc: 0.0943\n",
      "Epoch 35/100\n",
      "0s - loss: 0.0128 - acc: 0.9987 - val_loss: 4.9243 - val_acc: 0.0943\n",
      "Epoch 36/100\n",
      "0s - loss: 0.0115 - acc: 0.9980 - val_loss: 4.9439 - val_acc: 0.0943\n",
      "Epoch 37/100\n",
      "0s - loss: 0.0126 - acc: 0.9973 - val_loss: 4.9568 - val_acc: 0.0943\n",
      "Epoch 38/100\n",
      "0s - loss: 0.0123 - acc: 0.9973 - val_loss: 5.1972 - val_acc: 0.0943\n",
      "Epoch 39/100\n",
      "0s - loss: 0.0100 - acc: 0.9993 - val_loss: 4.9307 - val_acc: 0.0943\n",
      "Epoch 40/100\n",
      "0s - loss: 0.0099 - acc: 0.9993 - val_loss: 4.7430 - val_acc: 0.0943\n",
      "Epoch 41/100\n",
      "0s - loss: 0.0114 - acc: 0.9967 - val_loss: 4.8452 - val_acc: 0.0943\n",
      "Epoch 42/100\n",
      "0s - loss: 0.0070 - acc: 0.9993 - val_loss: 4.9760 - val_acc: 0.0943\n",
      "Epoch 43/100\n",
      "0s - loss: 0.0079 - acc: 0.9993 - val_loss: 4.9414 - val_acc: 0.0943\n",
      "Epoch 44/100\n",
      "0s - loss: 0.0119 - acc: 0.9980 - val_loss: 4.8422 - val_acc: 0.0943\n",
      "Epoch 45/100\n",
      "0s - loss: 0.0090 - acc: 0.9987 - val_loss: 4.9387 - val_acc: 0.0943\n",
      "Epoch 46/100\n",
      "0s - loss: 0.0085 - acc: 0.9993 - val_loss: 4.7996 - val_acc: 0.0943\n",
      "Epoch 47/100\n",
      "0s - loss: 0.0097 - acc: 0.9980 - val_loss: 4.5005 - val_acc: 0.0943\n",
      "Epoch 48/100\n",
      "0s - loss: 0.0082 - acc: 0.9980 - val_loss: 3.9677 - val_acc: 0.1246\n",
      "Epoch 49/100\n",
      "0s - loss: 0.0107 - acc: 0.9973 - val_loss: 3.6563 - val_acc: 0.1178\n",
      "Epoch 50/100\n",
      "0s - loss: 0.0080 - acc: 0.9980 - val_loss: 3.5149 - val_acc: 0.1178\n",
      "Epoch 51/100\n",
      "0s - loss: 0.0080 - acc: 0.9987 - val_loss: 3.4471 - val_acc: 0.1650\n",
      "Epoch 52/100\n",
      "0s - loss: 0.0111 - acc: 0.9980 - val_loss: 2.9515 - val_acc: 0.2054\n",
      "Epoch 53/100\n",
      "0s - loss: 0.0096 - acc: 0.9987 - val_loss: 2.8888 - val_acc: 0.2357\n",
      "Epoch 54/100\n",
      "0s - loss: 0.0103 - acc: 0.9987 - val_loss: 2.9963 - val_acc: 0.1987\n",
      "Epoch 55/100\n",
      "0s - loss: 0.0062 - acc: 1.0000 - val_loss: 2.8720 - val_acc: 0.2256\n",
      "Epoch 56/100\n",
      "0s - loss: 0.0117 - acc: 0.9953 - val_loss: 2.7734 - val_acc: 0.2828\n",
      "Epoch 57/100\n",
      "0s - loss: 0.0112 - acc: 0.9987 - val_loss: 2.0209 - val_acc: 0.4781\n",
      "Epoch 58/100\n",
      "0s - loss: 0.0112 - acc: 0.9973 - val_loss: 1.7545 - val_acc: 0.5253\n",
      "Epoch 59/100\n",
      "0s - loss: 0.0092 - acc: 0.9967 - val_loss: 1.5771 - val_acc: 0.5387\n",
      "Epoch 60/100\n",
      "0s - loss: 0.0044 - acc: 1.0000 - val_loss: 1.3644 - val_acc: 0.5993\n",
      "Epoch 61/100\n",
      "0s - loss: 0.0118 - acc: 0.9973 - val_loss: 1.1139 - val_acc: 0.6734\n",
      "Epoch 62/100\n",
      "0s - loss: 0.0118 - acc: 0.9973 - val_loss: 1.2021 - val_acc: 0.6498\n",
      "Epoch 63/100\n",
      "0s - loss: 0.0103 - acc: 0.9973 - val_loss: 1.0584 - val_acc: 0.7071\n",
      "Epoch 64/100\n",
      "0s - loss: 0.0090 - acc: 0.9980 - val_loss: 1.2767 - val_acc: 0.6700\n",
      "Epoch 65/100\n",
      "0s - loss: 0.0224 - acc: 0.9953 - val_loss: 1.4073 - val_acc: 0.6801\n",
      "Epoch 66/100\n",
      "0s - loss: 0.0197 - acc: 0.9947 - val_loss: 1.1114 - val_acc: 0.7239\n",
      "Epoch 67/100\n",
      "0s - loss: 0.0100 - acc: 0.9967 - val_loss: 0.9563 - val_acc: 0.7576\n",
      "Epoch 68/100\n",
      "0s - loss: 0.0099 - acc: 0.9980 - val_loss: 1.0555 - val_acc: 0.7542\n",
      "Epoch 69/100\n",
      "0s - loss: 0.0054 - acc: 0.9993 - val_loss: 1.1964 - val_acc: 0.7407\n",
      "Epoch 70/100\n",
      "0s - loss: 0.0085 - acc: 0.9973 - val_loss: 1.0023 - val_acc: 0.7677\n",
      "Epoch 71/100\n",
      "0s - loss: 0.0155 - acc: 0.9967 - val_loss: 0.5361 - val_acc: 0.8653\n",
      "Epoch 72/100\n",
      "0s - loss: 0.0099 - acc: 0.9973 - val_loss: 0.4724 - val_acc: 0.8822\n",
      "Epoch 73/100\n",
      "0s - loss: 0.0076 - acc: 0.9987 - val_loss: 0.4747 - val_acc: 0.8956\n",
      "Epoch 74/100\n",
      "0s - loss: 0.0068 - acc: 0.9993 - val_loss: 0.4757 - val_acc: 0.8855\n",
      "Epoch 75/100\n",
      "0s - loss: 0.0080 - acc: 0.9987 - val_loss: 0.4433 - val_acc: 0.8754\n",
      "Epoch 76/100\n",
      "0s - loss: 0.0080 - acc: 0.9973 - val_loss: 0.4322 - val_acc: 0.8788\n",
      "Epoch 77/100\n",
      "0s - loss: 0.0089 - acc: 0.9967 - val_loss: 0.4706 - val_acc: 0.8788\n",
      "Epoch 78/100\n",
      "0s - loss: 0.0068 - acc: 0.9987 - val_loss: 0.3864 - val_acc: 0.8990\n",
      "Epoch 79/100\n",
      "0s - loss: 0.0089 - acc: 0.9967 - val_loss: 0.2434 - val_acc: 0.9428\n",
      "Epoch 80/100\n",
      "0s - loss: 0.0048 - acc: 0.9993 - val_loss: 0.2704 - val_acc: 0.9394\n",
      "Epoch 81/100\n",
      "0s - loss: 0.0090 - acc: 0.9973 - val_loss: 0.2977 - val_acc: 0.9293\n",
      "Epoch 82/100\n",
      "0s - loss: 0.0064 - acc: 0.9987 - val_loss: 0.3550 - val_acc: 0.9226\n",
      "Epoch 83/100\n",
      "0s - loss: 0.0116 - acc: 0.9967 - val_loss: 0.3506 - val_acc: 0.9192\n",
      "Epoch 84/100\n",
      "0s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.4531 - val_acc: 0.9158\n",
      "Epoch 85/100\n",
      "0s - loss: 0.0081 - acc: 0.9973 - val_loss: 0.4808 - val_acc: 0.9125\n",
      "Epoch 86/100\n",
      "0s - loss: 0.0084 - acc: 0.9980 - val_loss: 0.5032 - val_acc: 0.9024\n",
      "Epoch 87/100\n",
      "0s - loss: 0.0045 - acc: 0.9993 - val_loss: 0.5541 - val_acc: 0.9024\n",
      "Epoch 88/100\n",
      "0s - loss: 0.0100 - acc: 0.9973 - val_loss: 0.5857 - val_acc: 0.8956\n",
      "Epoch 89/100\n",
      "0s - loss: 0.0090 - acc: 0.9967 - val_loss: 0.5393 - val_acc: 0.8956\n",
      "Epoch 90/100\n",
      "0s - loss: 0.0057 - acc: 0.9987 - val_loss: 0.4141 - val_acc: 0.9158\n",
      "Epoch 91/100\n",
      "0s - loss: 0.0198 - acc: 0.9927 - val_loss: 0.5423 - val_acc: 0.8956\n",
      "Epoch 92/100\n",
      "0s - loss: 0.0154 - acc: 0.9953 - val_loss: 0.4491 - val_acc: 0.9091\n",
      "Epoch 93/100\n",
      "0s - loss: 0.0121 - acc: 0.9973 - val_loss: 0.4383 - val_acc: 0.8990\n",
      "Epoch 94/100\n",
      "0s - loss: 0.0092 - acc: 0.9960 - val_loss: 0.4423 - val_acc: 0.9091\n",
      "Epoch 95/100\n",
      "0s - loss: 0.0147 - acc: 0.9973 - val_loss: 0.4202 - val_acc: 0.9259\n",
      "Epoch 96/100\n",
      "0s - loss: 0.0163 - acc: 0.9967 - val_loss: 0.4158 - val_acc: 0.9293\n",
      "Epoch 97/100\n",
      "0s - loss: 0.0144 - acc: 0.9973 - val_loss: 0.5410 - val_acc: 0.9091\n",
      "Epoch 98/100\n",
      "0s - loss: 0.0120 - acc: 0.9953 - val_loss: 0.5437 - val_acc: 0.9091\n",
      "Epoch 99/100\n",
      "0s - loss: 0.0055 - acc: 0.9980 - val_loss: 0.6264 - val_acc: 0.9024\n",
      "Epoch 100/100\n",
      "0s - loss: 0.0111 - acc: 0.9980 - val_loss: 0.5620 - val_acc: 0.9057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75292ec450>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=200, verbose=2, \n",
    "          callbacks=[TensorBoard(log_dir='/tmp/autoencoder/Digits_5layers_Dropout_BatchNormalization')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
